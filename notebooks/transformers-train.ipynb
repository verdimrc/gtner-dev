{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:250%; font-weight:bold'>Train NER with huggingface/transformers</div>\n",
    "\n",
    "This notebook shows how to use `huggingface/transformers` on Amazon SageMaker to transfer-learn the Roberta language model into a new NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import s3fs\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "from gtner_blog.util import split, bilou2bio, write_split, LabelCollector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download transformers NER scripts\n",
    "\n",
    "The `huggingface/transformers` repo contains two PyTorch scripts to download, namely `run_ner.py` and `utils_ner.py`. The following `bash` cell downloads version v2.5.0 which matches the library listed in `requirements.txt`.\n",
    "\n",
    "To minimize the dependencies installed to the training container, we also download the `seqeval` into `source_dir/` to emulate `pip install --no-deps seqeval`. The `seqeval.callbacks` depends on [Keras](https://github.com/chakki-works/seqeval/blob/v0.0.12/seqeval/callbacks.py) and [tensorflow](https://github.com/chakki-works/seqeval/blob/v0.0.12/requirements.txt), but `run_ner.py` does not uses these callbacks (and only `seqeval.metrics`), hence both dependencies can be [skipped](https://github.com/chakki-works/seqeval/blob/v0.0.12/seqeval/metrics/sequence_labeling.py).\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Note</summary>\n",
    "    <blockquote>As of this writing, the master branch of `huggingface/transformers` has relocated the NER scripts from `examples/` to `examples/ner/`, which is beyond the scope of this notebook.</blockquote>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ec2-user ec2-user  3111 Feb 21 10:37 callbacks.py\n",
      "-rw-rw-r-- 1 ec2-user ec2-user     0 Feb 21 10:37 __init__.py\n",
      "-rw-rw-r-- 1 ec2-user ec2-user    32 Feb 21 07:38 requirements.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 30349 Feb 21 10:40 \u001b[01;31m\u001b[Krun_ner.py\u001b[m\u001b[K\n",
      "drwxrwxr-x 3 ec2-user ec2-user  4096 Feb 21 08:12 \u001b[01;31m\u001b[Kseqeval\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  3111 Feb 21 10:40 \u001b[01;31m\u001b[Kseqeval/callbacks.py\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user     0 Feb 21 10:40 \u001b[01;31m\u001b[Kseqeval/__init__.py\u001b[m\u001b[K\n",
      "drwxrwxr-x 2 ec2-user ec2-user  4096 Feb 21 08:12 \u001b[01;31m\u001b[Kseqeval/metrics\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   371 Feb 21 10:40 \u001b[01;31m\u001b[Kseqeval/metrics/__init__.py\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 12604 Feb 21 10:40 \u001b[01;31m\u001b[Kseqeval/metrics/sequence_labeling.py\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  2287 Feb 21 08:37 transformers-train.py\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  8428 Feb 21 10:40 \u001b[01;31m\u001b[Kutils_ner.py\u001b[m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "GITHUB=https://raw.githubusercontent.com\n",
    "cd transformers-scripts\n",
    "\n",
    "# Download NER scripts\n",
    "for i in run_ner.py utils_ner.py\n",
    "do\n",
    "    curl --silent --location $GITHUB/huggingface/transformers/v2.5.0/examples/$i > $i\n",
    "done\n",
    "\n",
    "# Download seqeval\n",
    "mkdir -p seqeval/metrics\n",
    "for i in __init__.py callbacks.py metrics/__init__.py metrics/sequence_labeling.py\n",
    "do\n",
    "    curl --silent --location $GITHUB/chakki-works/seqeval/v0.0.12/seqeval/$i > seqeval/$i\n",
    "done\n",
    "\n",
    "ls -ald * seqeval/* seqeval/metrics/* | egrep --color=always 'run_ner.py|utils_ner.py|seqeval.*|^'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data channels\n",
    "\n",
    "Split the whole corpus in S3 into train:test = 3:1 proportion, then upload the splits to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/gt/test-gtner-blog-004/manifests/output/output.iob'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/transformers-data/train'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/transformers-data/dev'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/transformers-data/label'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket = 'gtner-blog'                # Change me as necessary\n",
    "gt_jobname = 'test-gtner-blog-004'   # Change me as necessary\n",
    "\n",
    "iob_file = f's3://{bucket}/gt/{gt_jobname}/manifests/output/output.iob'\n",
    "train = f's3://{bucket}/transformers-data/train'\n",
    "dev = f's3://{bucket}/transformers-data/dev'\n",
    "label = f's3://{bucket}/transformers-data/label'\n",
    "label_collector = LabelCollector()\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "with fs.open(iob_file, 'r') as f:\n",
    "    train_split = os.path.join(train, 'train.txt')\n",
    "    test_split = os.path.join(dev, 'dev.txt')\n",
    "    \n",
    "    # Chain of functions: .iob -> bilou2bio -> label_collector -> split -> write_split.\n",
    "    write_split(split(label_collector(bilou2bio(f))), train_split, test_split)\n",
    "\n",
    "with fs.open(os.path.join(label, 'label.txt'), 'w') as f:\n",
    "    for ner_tag in label_collector.sorted_labels:\n",
    "        f.write(f'{ner_tag}\\n')\n",
    "\n",
    "display(iob_file, train, dev, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training\n",
    "\n",
    "We create a PyTorch estimator with our entry point script `transformers-train.py`, a thin wrapper over `run_ner.py` that does the following:\n",
    "\n",
    "1. parse SageMaker's entry-point protocol, namely model and channel directories.\n",
    "2. pre-define a few arguments to `run-ner.py`: `{\"--do_train\", \"--do-eval\", \"--evaluate_during_train\", \"--data_dir\", \"--output_dir\", \"--label\"}`.\n",
    "3. passes the estimator's hyper-parameters as arguments to `run-ner.py`.\n",
    "   1. Each hyperparameter `abcd` will be passed down as `--abcd`.\n",
    "   2. The hyperparameters must not conflict with those in the above mentioned step 2.\n",
    "   3. The entry point only support `--abcd SOME_VALUE` form of arguments.\n",
    "\n",
    "```bash\n",
    "usage: run_ner.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\n",
    "                  --model_name_or_path MODEL_NAME_OR_PATH --output_dir\n",
    "                  OUTPUT_DIR [--labels LABELS] [--config_name CONFIG_NAME]\n",
    "                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
    "                  [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\n",
    "                  [--do_predict] [--evaluate_during_training]\n",
    "                  [--do_lower_case]\n",
    "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
    "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
    "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
    "                  [--learning_rate LEARNING_RATE]\n",
    "                  [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n",
    "                  [--max_grad_norm MAX_GRAD_NORM]\n",
    "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
    "                  [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n",
    "                  [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
    "                  [--eval_all_checkpoints] [--no_cuda]\n",
    "                  [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n",
    "                  [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n",
    "                  [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n",
    "                  [--server_port SERVER_PORT]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='transformers-train.py',\n",
    "                    source_dir='./transformers-scripts',\n",
    "                    role=get_execution_role(),\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m5.large',\n",
    "                    framework_version='1.3.1',\n",
    "                    py_version='py3',\n",
    "                    debugger_hook_config=False,\n",
    "                    hyperparameters={\n",
    "                        'num_train_epochs': 5.0,\n",
    "                        'model_type': 'roberta',\n",
    "                        'model_name_or_path': 'roberta-base'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-21 10:42:22 Starting - Starting the training job...\n",
      "2020-02-21 10:42:25 Starting - Launching requested ML instances......\n",
      "2020-02-21 10:43:24 Starting - Preparing the instances for training...\n",
      "2020-02-21 10:44:06 Downloading - Downloading input data...\n",
      "2020-02-21 10:44:35 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-02-21 10:44:57,436 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-02-21 10:44:57,439 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-21 10:44:57,451 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-02-21 10:44:58,865 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-02-21 10:44:59,077 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-02-21 10:44:59,077 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-02-21 10:44:59,078 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-02-21 10:44:59,078 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpbptt1s86/module_dir\u001b[0m\n",
      "\u001b[34mCollecting transformers==2.5.0\n",
      "  Downloading transformers-2.5.0-py3-none-any.whl (481 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\n",
      "  Downloading tensorboard-2.1.0-py3-none-any.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.2.20-cp36-cp36m-manylinux2010_x86_64.whl (690 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (4.36.1)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.5.0\n",
      "  Downloading tokenizers-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (1.11.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (3.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (1.12.0)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (0.33.6)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.11.2-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (0.15.5)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl (2.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (44.0.0.post20200106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.5.0->-r requirements.txt (line 1)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.5.0->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.15.0,>=1.14.7 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.5.0->-r requirements.txt (line 1)) (1.14.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.5.0->-r requirements.txt (line 1)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.5.0->-r requirements.txt (line 1)) (0.9.4)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (3.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.5.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.5.0->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (0.4.8)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, sacremoses, absl-py\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=36740 sha256=881ac65c653d846348601f985f1ac6e4ecdda4eaea5b408d55d65325b4718878\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-aalkmc82/wheels/47/3d/07/727f4571aee0e00d3099a53aabe2dc80db8f410b2ecf8298a2\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=4f3d7cf4c13c95c9a532e68fe63bef45ba55f9324a9342bee40c0b1e7aae5df2\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=4b39165b82731f741b5e129b9d35adb36f24b90b58bf993f37ef8ed9d1431d70\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name sacremoses absl-py\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, sentencepiece, tokenizers, filelock, sacremoses, transformers, absl-py, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, markdown, grpcio, tensorboard, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-0.9.0 cachetools-4.0.0 default-user-module-name-1.0.0 filelock-3.0.12 google-auth-1.11.2 google-auth-oauthlib-0.4.1 grpcio-1.27.2 markdown-3.2.1 oauthlib-3.1.0 pyasn1-modules-0.2.8 regex-2020.2.20 requests-oauthlib-1.3.0 sacremoses-0.0.38 sentencepiece-0.1.85 tensorboard-2.1.0 tokenizers-0.5.0 transformers-2.5.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-02-21 10:45:06,847 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-21 10:45:06,860 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-21 10:45:06,874 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-21 10:45:06,885 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"dev\": \"/opt/ml/input/data/dev\",\n",
      "        \"label\": \"/opt/ml/input/data/label\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"num_train_epochs\": 5.0,\n",
      "        \"model_type\": \"roberta\",\n",
      "        \"model_name_or_path\": \"roberta-base\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"dev\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"label\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-02-21-10-42-21-545\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-21-10-42-21-545/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transformers-train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transformers-train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_name_or_path\":\"roberta-base\",\"model_type\":\"roberta\",\"num_train_epochs\":5.0}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transformers-train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"dev\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"label\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"dev\",\"label\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transformers-train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-21-10-42-21-545/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"dev\":\"/opt/ml/input/data/dev\",\"label\":\"/opt/ml/input/data/label\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_name_or_path\":\"roberta-base\",\"model_type\":\"roberta\",\"num_train_epochs\":5.0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"dev\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"label\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-02-21-10-42-21-545\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-21-10-42-21-545/source/sourcedir.tar.gz\",\"module_name\":\"transformers-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transformers-train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_name_or_path\",\"roberta-base\",\"--model_type\",\"roberta\",\"--num_train_epochs\",\"5.0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DEV=/opt/ml/input/data/dev\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_LABEL=/opt/ml/input/data/label\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=5.0\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_TYPE=roberta\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=roberta-base\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python transformers-train.py --model_name_or_path roberta-base --model_type roberta --num_train_epochs 5.0\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2020-02-21 10:44:56 Training - Training image download completed. Training in progress.\u001b[34m['run_ner.py', '--do_train', '--do_eval', '--evaluate_during_train', '--data_dir', '/opt/ml/input/data/train', '--output_dir', '/opt/ml/model', '--label', '/opt/ml/input/data/label/label.txt', '--model_name_or_path', 'roberta-base', '--model_type', 'roberta', '--num_train_epochs', '5.0']\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:08 - WARNING - run_ner -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\u001b[0m\n",
      "\n",
      "2020-02-21 10:46:53 Uploading - Uploading generated training model\u001b[34m02/21/2020 10:45:09 - INFO - filelock -   Lock 139662613216728 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:09 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpvm02khev\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/524 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 524/524 [00:00<00:00, 458kB/s]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - filelock -   Lock 139662613216728 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:11 - INFO - filelock -   Lock 139662613216728 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:11 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpunubg3ci\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]#015Downloading:   2%|▏         | 17.4k/899k [00:00<00:10, 83.0kB/s]#015Downloading:   6%|▌         | 52.2k/899k [00:00<00:08, 97.5kB/s]#015Downloading:  14%|█▎        | 122k/899k [00:00<00:06, 124kB/s]  #015Downloading:  29%|██▉       | 261k/899k [00:00<00:03, 164kB/s]#015Downloading:  64%|██████▍   | 573k/899k [00:01<00:01, 223kB/s]#015Downloading: 100%|██████████| 899k/899k [00:01<00:00, 849kB/s]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:13 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:13 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:13 - INFO - filelock -   Lock 139662613216728 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:14 - INFO - filelock -   Lock 139662332852600 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:14 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpc68i9ccp\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]#015Downloading:   4%|▍         | 17.4k/456k [00:00<00:05, 82.3kB/s]#015Downloading:  11%|█▏        | 52.2k/456k [00:00<00:04, 97.0kB/s]#015Downloading:  27%|██▋       | 122k/456k [00:00<00:02, 123kB/s]  #015Downloading:  57%|█████▋    | 261k/456k [00:00<00:01, 163kB/s]#015Downloading: 100%|██████████| 456k/456k [00:00<00:00, 538kB/s]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - filelock -   Lock 139662332852600 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:17 - INFO - filelock -   Lock 139662332854000 acquired on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:17 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpdb7xyv5y\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]#015Downloading:   0%|          | 1.02k/501M [00:00<29:21:04, 4.74kB/s]#015Downloading:   0%|          | 34.8k/501M [00:00<20:48:44, 6.69kB/s]#015Downloading:   0%|          | 104k/501M [00:00<14:41:48, 9.47kB/s] #015Downloading:   0%|          | 225k/501M [00:00<10:21:36, 13.4kB/s]#015Downloading:   0%|          | 469k/501M [00:01<7:17:08, 19.1kB/s] #015Downloading:   0%|          | 957k/501M [00:01<5:06:48, 27.2kB/s]#015Downloading:   0%|          | 1.93M/501M [00:01<3:34:54, 38.7kB/s]#015Downloading:   1%|          | 3.51M/501M [00:01<2:30:18, 55.2kB/s]#015Downloading:   1%|          | 5.08M/501M [00:01<1:45:13, 78.6kB/s]#015Downloading:   1%|▏         | 6.65M/501M [00:02<1:13:45, 112kB/s] #015Downloading:   2%|▏         | 8.22M/501M [00:02<51:48, 159kB/s]  #015Downloading:   2%|▏         | 9.80M/501M [00:02<36:29, 224kB/s]#015Downloading:   2%|▏         | 11.4M/501M [00:02<25:47, 316kB/s]#015Downloading:   3%|▎         | 12.9M/501M [00:03<18:20, 444kB/s]#015Downloading:   3%|▎         | 14.5M/501M [00:03<13:07, 618kB/s]#015Downloading:   3%|▎         | 16.1M/501M [00:03<09:29, 852kB/s]#015Downloading:   4%|▎         | 17.7M/501M [00:03<06:57, 1.16MB/s]#015Downloading:   4%|▍         | 19.3M/501M [00:03<05:11, 1.55MB/s]#015Downloading:   4%|▍         | 20.8M/501M [00:04<03:56, 2.03MB/s]#015Downloading:   4%|▍         | 22.4M/501M [00:04<03:05, 2.59MB/s]#015Downloading:   5%|▍         | 24.0M/501M [00:04<02:28, 3.20MB/s]#015Downloading:   5%|▌         | 25.5M/501M [00:04<02:03, 3.85MB/s]#015Downloading:   5%|▌         | 27.1M/501M [00:04<01:45, 4.49MB/s]#015Downloading:   6%|▌         | 28.7M/501M [00:05<01:33, 5.07MB/s]#015Downloading:   6%|▌         | 30.3M/501M [00:05<01:24, 5.57MB/s]#015Downloading:   6%|▋         | 31.9M/501M [00:05<01:18, 5.99MB/s]#015Downloading:   7%|▋         | 33.4M/501M [00:05<01:13, 6.32MB/s]#015Downloading:   7%|▋         | 35.0M/501M [00:06<01:10, 6.58MB/s]#015Downloading:   7%|▋         | 36.6M/501M [00:06<01:08, 6.77MB/s]#015Downloading:   8%|▊         | 38.1M/501M [00:06<01:07, 6.90MB/s]#015Downloading:   8%|▊         | 39.7M/501M [00:06<01:05, 7.01MB/s]#015Downloading:   8%|▊         | 41.3M/501M [00:06<01:04, 7.10MB/s]#015Downloading:   9%|▊         | 42.9M/501M [00:07<01:04, 7.15MB/s]#015Downloading:   9%|▉         | 44.4M/501M [00:07<01:03, 7.18MB/s]#015Downloading:   9%|▉         | 46.0M/501M [00:07<01:03, 7.20MB/s]#015Downloading:   9%|▉         | 47.6M/501M [00:07<01:02, 7.22MB/s]#015Downloading:  10%|▉         | 49.2M/501M [00:08<01:02, 7.23MB/s]#015Downloading:  10%|█         | 50.7M/501M [00:08<01:02, 7.24MB/s]#015Downloading:  10%|█         | 52.3M/501M [00:08<01:01, 7.24MB/s]#015Downloading:  11%|█         | 53.9M/501M [00:08<01:01, 7.27MB/s]#015Downloading:  11%|█         | 55.5M/501M [00:08<01:01, 7.26MB/s]#015Downloading:  11%|█▏        | 57.0M/501M [00:09<01:01, 7.26MB/s]#015Downloading:  12%|█▏        | 58.6M/501M [00:09<01:00, 7.26MB/s]#015Downloading:  12%|█▏        | 60.2M/501M [00:09<01:00, 7.26MB/s]#015Downloading:  12%|█▏        | 61.8M/501M [00:09<01:00, 7.26MB/s]#015Downloading:  13%|█▎        | 63.3M/501M [00:09<01:00, 7.26MB/s]#015Downloading:  13%|█▎        | 64.9M/501M [00:10<00:59, 7.28MB/s]#015Downloading:  13%|█▎        | 66.5M/501M [00:10<00:59, 7.27MB/s]#015Downloading:  14%|█▎        | 68.1M/501M [00:10<00:59, 7.27MB/s]#015Downloading:  14%|█▍        | 69.6M/501M [00:10<00:59, 7.26MB/s]#015Downloading:  14%|█▍        | 71.2M/501M [00:11<00:59, 7.26MB/s]#015Downloading:  15%|█▍        | 72.8M/501M [00:11<00:59, 7.26MB/s]#015Downloading:  15%|█▍        | 74.4M/501M [00:11<00:58, 7.26MB/s]#015Downloading:  15%|█▌        | 76.0M/501M [00:11<00:58, 7.28MB/s]#015Downloading:  15%|█▌        | 77.5M/501M [00:11<00:58, 7.27MB/s]#015Downloading:  16%|█▌        | 79.1M/501M [00:12<00:58, 7.27MB/s]#015Downloading:  16%|█▌        | 80.7M/501M [00:12<00:57, 7.26MB/s]#015Downloading:  16%|█▋        | 82.2M/501M [00:12<00:57, 7.26MB/s]#015Downloading:  17%|█▋        | 83.8M/501M [00:12<00:57, 7.26MB/s]#015Downloading:  17%|█▋        | 85.4M/501M [00:13<00:57, 7.26MB/s]#015Downloading:  17%|█▋        | 87.0M/501M [00:13<00:57, 7.26MB/s]#015Downloading:  18%|█▊        | 88.5M/501M [00:13<00:47, 8.63MB/s]#015Downloading:  18%|█▊        | 89.5M/501M [00:13<00:50, 8.22MB/s]#015Downloading:  18%|█▊        | 90.4M/501M [00:13<01:03, 6.51MB/s]#015Downloading:  18%|█▊        | 91.7M/501M [00:13<01:04, 6.38MB/s]#015Downloading:  19%|█▊        | 93.3M/501M [00:14<01:01, 6.62MB/s]#015Downloading:  19%|█▉        | 94.8M/501M [00:14<00:59, 6.80MB/s]#015Downloading:  19%|█▉        | 96.4M/501M [00:14<00:58, 6.93MB/s]#015Downloading:  20%|█▉        | 98.0M/501M [00:14<00:57, 7.02MB/s]#015Downloading:  20%|█▉        | 99.6M/501M [00:14<00:56, 7.09MB/s]#015Downloading:  20%|██        | 101M/501M [00:15<00:55, 7.16MB/s] #015Downloading:  20%|██        | 103M/501M [00:15<00:55, 7.19MB/s]#015Downloading:  21%|██        | 104M/501M [00:15<00:55, 7.21MB/s]#015Downloading:  21%|██        | 106M/501M [00:15<00:54, 7.22MB/s]#015Downloading:  21%|██▏       | 107M/501M [00:16<00:54, 7.23MB/s]#015Downloading:  22%|██▏       | 109M/501M [00:16<00:54, 7.24MB/s]#015Downloading:  22%|██▏       | 111M/501M [00:16<00:53, 7.24MB/s]#015Downloading:  22%|██▏       | 112M/501M [00:16<00:53, 7.25MB/s]#015Downloading:  23%|██▎       | 114M/501M [00:16<00:53, 7.27MB/s]#015Downloading:  23%|██▎       | 115M/501M [00:17<00:53, 7.27MB/s]#015Downloading:  23%|██▎       | 117M/501M [00:17<00:52, 7.26MB/s]#015Downloading:  24%|██▎       | 118M/501M [00:17<00:52, 7.26MB/s]#015Downloading:  24%|██▍       | 120M/501M [00:17<00:52, 7.26MB/s]#015Downloading:  24%|██▍       | 122M/501M [00:17<00:52, 7.26MB/s]#015Downloading:  25%|██▍       | 123M/501M [00:18<00:52, 7.26MB/s]#015Downloading:  25%|██▍       | 125M/501M [00:18<00:51, 7.26MB/s]#015Downloading:  25%|██▌       | 126M/501M [00:18<00:51, 7.26MB/s]#015Downloading:  26%|██▌       | 128M/501M [00:18<00:51, 7.26MB/s]#015Downloading:  26%|██▌       | 129M/501M [00:19<00:51, 7.28MB/s]#015Downloading:  26%|██▌       | 131M/501M [00:19<00:50, 7.27MB/s]#015Downloading:  26%|██▋       | 133M/501M [00:19<00:50, 7.27MB/s]#015Downloading:  27%|██▋       | 134M/501M [00:19<00:50, 7.26MB/s]#015Downloading:  27%|██▋       | 136M/501M [00:19<00:50, 7.26MB/s]#015Downloading:  27%|██▋       | 137M/501M [00:20<00:50, 7.26MB/s]#015Downloading:  28%|██▊       | 139M/501M [00:20<00:49, 7.26MB/s]#015Downloading:  28%|██▊       | 141M/501M [00:20<00:49, 7.26MB/s]#015Downloading:  28%|██▊       | 142M/501M [00:20<00:49, 7.28MB/s]#015Downloading:  29%|██▊       | 144M/501M [00:21<00:49, 7.27MB/s]#015Downloading:  29%|██▉       | 145M/501M [00:21<00:48, 7.27MB/s]#015Downloading:  29%|██▉       | 147M/501M [00:21<00:48, 7.26MB/s]#015Downloading:  30%|██▉       | 148M/501M [00:21<00:48, 7.26MB/s]#015Downloading:  30%|██▉       | 150M/501M [00:21<00:48, 7.26MB/s]#015Downloading:  30%|███       | 152M/501M [00:22<00:48, 7.26MB/s]#015Downloading:  31%|███       | 153M/501M [00:22<00:47, 7.28MB/s]#015Downloading:  31%|███       | 155M/501M [00:22<00:47, 7.27MB/s]#015Downloading:  31%|███       | 156M/501M [00:22<00:47, 7.27MB/s]#015Downloading:  31%|███▏      | 158M/501M [00:22<00:47, 7.26MB/s]#015Downloading:  32%|███▏      | 159M/501M [00:23<00:47, 7.26MB/s]#015Downloading:  32%|███▏      | 161M/501M [00:23<00:46, 7.26MB/s]#015Downloading:  32%|███▏      | 163M/501M [00:23<00:46, 7.26MB/s]#015Downloading:  33%|███▎      | 164M/501M [00:23<00:46, 7.26MB/s]#015Downloading:  33%|███▎      | 166M/501M [00:24<00:46, 7.28MB/s]#015Downloading:  33%|███▎      | 167M/501M [00:24<00:45, 7.27MB/s]#015Downloading:  34%|███▎      | 169M/501M [00:24<00:45, 7.27MB/s]#015Downloading:  34%|███▍      | 170M/501M [00:24<00:45, 7.26MB/s]#015Downloading:  34%|███▍      | 172M/501M [00:24<00:45, 7.26MB/s]#015Downloading:  35%|███▍      | 174M/501M [00:25<00:45, 7.26MB/s]#015Downloading:  35%|███▍      | 175M/501M [00:25<00:44, 7.26MB/s]#015Downloading:  35%|███▌      | 177M/501M [00:25<00:44, 7.26MB/s]#015Downloading:  36%|███▌      | 178M/501M [00:25<00:44, 7.26MB/s]#015Downloading:  36%|███▌      | 180M/501M [00:26<00:44, 7.28MB/s]#015Downloading:  36%|███▌      | 181M/501M [00:26<00:43, 7.27MB/s]#015Downloading:  37%|███▋      | 183M/501M [00:26<00:43, 7.27MB/s]#015Downloading:  37%|███▋      | 185M/501M [00:26<00:43, 7.26MB/s]#015Downloading:  37%|███▋      | 186M/501M [00:26<00:43, 7.26MB/s]#015Downloading:  37%|███▋      | 188M/501M [00:27<00:43, 7.26MB/s]#015Downloading:  38%|███▊      | 189M/501M [00:27<00:42, 7.26MB/s]#015Downloading:  38%|███▊      | 191M/501M [00:27<00:42, 7.26MB/s]#015Downloading:  38%|███▊      | 192M/501M [00:27<00:42, 7.28MB/s]#015Downloading:  39%|███▊      | 194M/501M [00:27<00:42, 7.27MB/s]#015Downloading:  39%|███▉      | 196M/501M [00:28<00:42, 7.27MB/s]#015Downloading:  39%|███▉      | 197M/501M [00:28<00:41, 7.26MB/s]#015Downloading:  40%|███▉      | 199M/501M [00:28<00:41, 7.26MB/s]#015Downloading:  40%|███▉      | 200M/501M [00:28<00:41, 7.26MB/s]#015Downloading:  40%|████      | 202M/501M [00:29<00:41, 7.26MB/s]#015Downloading:  41%|████      | 204M/501M [00:29<00:40, 7.28MB/s]#015Downloading:  41%|████      | 205M/501M [00:29<00:40, 7.27MB/s]#015Downloading:  41%|████      | 207M/501M [00:29<00:40, 7.27MB/s]#015Downloading:  42%|████▏     | 208M/501M [00:29<00:40, 7.26MB/s]#015Downloading:  42%|████▏     | 210M/501M [00:30<00:40, 7.26MB/s]#015Downloading:  42%|████▏     | 211M/501M [00:30<00:39, 7.26MB/s]#015Downloading:  42%|████▏     | 213M/501M [00:30<00:39, 7.26MB/s]#015Downloading:  43%|████▎     | 215M/501M [00:30<00:39, 7.26MB/s]#015Downloading:  43%|████▎     | 216M/501M [00:31<00:39, 7.26MB/s]#015Downloading:  43%|████▎     | 218M/501M [00:31<00:38, 7.28MB/s]#015Downloading:  44%|████▎     | 219M/501M [00:31<00:38, 7.27MB/s]#015Downloading:  44%|████▍     | 221M/501M [00:31<00:38, 7.27MB/s]#015Downloading:  44%|████▍     | 222M/501M [00:31<00:38, 7.26MB/s]#015Downloading:  45%|████▍     | 224M/501M [00:32<00:38, 7.26MB/s]#015Downloading:  45%|████▌     | 226M/501M [00:32<00:37, 7.26MB/s]#015Downloading:  45%|████▌     | 227M/501M [00:32<00:37, 7.26MB/s]#015Downloading:  46%|████▌     | 229M/501M [00:32<00:37, 7.28MB/s]#015Downloading:  46%|████▌     | 230M/501M [00:32<00:37, 7.27MB/s]#015Downloading:  46%|████▋     | 232M/501M [00:33<00:37, 7.27MB/s]#015Downloading:  47%|████▋     | 233M/501M [00:33<00:36, 7.26MB/s]#015Downloading:  47%|████▋     | 235M/501M [00:33<00:36, 7.26MB/s]#015Downloading:  47%|████▋     | 237M/501M [00:33<00:36, 7.26MB/s]#015Downloading:  48%|████▊     | 238M/501M [00:34<00:36, 7.26MB/s]#015Downloading:  48%|████▊     | 240M/501M [00:34<00:35, 7.28MB/s]#015Downloading:  48%|████▊     | 241M/501M [00:34<00:35, 7.26MB/s]#015Downloading:  48%|████▊     | 243M/501M [00:34<00:35, 7.27MB/s]#015Downloading:  49%|████▉     | 244M/501M [00:34<00:35, 7.27MB/s]#015Downloading:  49%|████▉     | 246M/501M [00:35<00:35, 7.26MB/s]#015Downloading:  49%|████▉     | 248M/501M [00:35<00:34, 7.26MB/s]#015Downloading:  50%|████▉     | 249M/501M [00:35<00:34, 7.26MB/s]#015Downloading:  50%|█████     | 251M/501M [00:35<00:34, 7.26MB/s]#015Downloading:  50%|█████     | 252M/501M [00:35<00:34, 7.28MB/s]#015Downloading:  51%|█████     | 254M/501M [00:36<00:34, 7.27MB/s]#015Downloading:  51%|█████     | 255M/501M [00:36<00:33, 7.27MB/s]#015Downloading:  51%|█████▏    | 257M/501M [00:36<00:33, 7.26MB/s]#015Downloading:  52%|█████▏    | 259M/501M [00:36<00:33, 7.26MB/s]#015Downloading:  52%|█████▏    | 260M/501M [00:37<00:33, 7.26MB/s]#015Downloading:  52%|█████▏    | 262M/501M [00:37<00:32, 7.26MB/s]#015Downloading:  53%|█████▎    | 263M/501M [00:37<00:32, 7.26MB/s]#015Downloading:  53%|█████▎    | 265M/501M [00:37<00:32, 7.28MB/s]#015Downloading:  53%|█████▎    | 267M/501M [00:37<00:32, 7.27MB/s]#015Downloading:  53%|█████▎    | 268M/501M [00:38<00:32, 7.27MB/s]#015Downloading:  54%|█████▍    | 270M/501M [00:38<00:31, 7.26MB/s]#015Downloading:  54%|█████▍    | 271M/501M [00:38<00:31, 7.26MB/s]#015Downloading:  54%|█████▍    | 273M/501M [00:38<00:31, 7.26MB/s]#015Downloading:  55%|█████▍    | 274M/501M [00:39<00:31, 7.25MB/s]#015Downloading:  55%|█████▌    | 276M/501M [00:39<00:31, 7.25MB/s]#015Downloading:  55%|█████▌    | 278M/501M [00:39<00:30, 7.28MB/s]#015Downloading:  56%|█████▌    | 279M/501M [00:39<00:30, 7.27MB/s]#015Downloading:  56%|█████▌    | 281M/501M [00:39<00:30, 7.27MB/s]#015Downloading:  56%|█████▋    | 282M/501M [00:40<00:30, 7.26MB/s]#015Downloading:  57%|█████▋    | 284M/501M [00:40<00:29, 7.26MB/s]#015Downloading:  57%|█████▋    | 285M/501M [00:40<00:29, 7.26MB/s]#015Downloading:  57%|█████▋    | 287M/501M [00:40<00:29, 7.25MB/s]#015Downloading:  58%|█████▊    | 289M/501M [00:40<00:29, 7.28MB/s]#015Downloading:  58%|█████▊    | 290M/501M [00:41<00:29, 7.27MB/s]#015Downloading:  58%|█████▊    | 292M/501M [00:41<00:28, 7.27MB/s]#015Downloading:  59%|█████▊    | 293M/501M [00:41<00:28, 7.26MB/s]#015Downloading:  59%|█████▉    | 295M/501M [00:41<00:28, 7.26MB/s]#015Downloading:  59%|█████▉    | 296M/501M [00:42<00:28, 7.26MB/s]#015Downloading:  59%|█████▉    | 298M/501M [00:42<00:27, 7.26MB/s]#015Downloading:  60%|█████▉    | 300M/501M [00:42<00:27, 7.27MB/s]#015Downloading:  60%|██████    | 301M/501M [00:42<00:27, 7.27MB/s]#015Downloading:  60%|██████    | 303M/501M [00:42<00:27, 7.27MB/s]#015Downloading:  61%|██████    | 304M/501M [00:43<00:27, 7.26MB/s]#015Downloading:  61%|██████    | 306M/501M [00:43<00:26, 7.26MB/s]#015Downloading:  61%|██████▏   | 307M/501M [00:43<00:26, 7.26MB/s]#015Downloading:  62%|██████▏   | 309M/501M [00:43<00:26, 7.26MB/s]#015Downloading:  62%|██████▏   | 311M/501M [00:44<00:26, 7.26MB/s]#015Downloading:  62%|██████▏   | 312M/501M [00:44<00:24, 7.62MB/s]#015Downloading:  63%|██████▎   | 314M/501M [00:44<00:21, 8.53MB/s]#015Downloading:  63%|██████▎   | 315M/501M [00:44<00:24, 7.56MB/s]#015Downloading:  63%|██████▎   | 315M/501M [00:44<00:30, 6.12MB/s]#015Downloading:  63%|██████▎   | 317M/501M [00:44<00:29, 6.33MB/s]#015Downloading:  64%|██████▎   | 318M/501M [00:45<00:27, 6.58MB/s]#015Downloading:  64%|██████▍   | 320M/501M [00:45<00:26, 6.77MB/s]#015Downloading:  64%|██████▍   | 322M/501M [00:45<00:25, 6.91MB/s]#015Downloading:  64%|██████▍   | 323M/501M [00:45<00:25, 7.01MB/s]#015Downloading:  65%|██████▍   | 325M/501M [00:45<00:24, 7.08MB/s]#015Downloading:  65%|██████▌   | 326M/501M [00:46<00:24, 7.15MB/s]#015Downloading:  65%|██████▌   | 328M/501M [00:46<00:24, 7.18MB/s]#015Downloading:  66%|██████▌   | 330M/501M [00:46<00:23, 7.20MB/s]#015Downloading:  66%|██████▌   | 331M/501M [00:46<00:23, 7.22MB/s]#015Downloading:  66%|██████▋   | 333M/501M [00:47<00:23, 7.23MB/s]#015Downloading:  67%|██████▋   | 334M/501M [00:47<00:23, 7.24MB/s]#015Downloading:  67%|██████▋   | 336M/501M [00:47<00:22, 7.24MB/s]#015Downloading:  67%|██████▋   | 337M/501M [00:47<00:22, 7.25MB/s]#015Downloading:  68%|██████▊   | 339M/501M [00:47<00:22, 7.27MB/s]#015Downloading:  68%|██████▊   | 341M/501M [00:48<00:22, 7.27MB/s]#015Downloading:  68%|██████▊   | 342M/501M [00:48<00:21, 7.26MB/s]#015Downloading:  69%|██████▊   | 344M/501M [00:48<00:21, 7.26MB/s]#015Downloading:  69%|██████▉   | 345M/501M [00:48<00:21, 7.26MB/s]#015Download\u001b[0m\n",
      "\u001b[34ming:  69%|██████▉   | 347M/501M [00:48<00:21, 7.26MB/s]#015Downloading:  70%|██████▉   | 348M/501M [00:49<00:21, 7.26MB/s]#015Downloading:  70%|██████▉   | 350M/501M [00:49<00:20, 7.26MB/s]#015Downloading:  70%|███████   | 352M/501M [00:49<00:20, 7.26MB/s]#015Downloading:  70%|███████   | 353M/501M [00:49<00:20, 7.28MB/s]#015Downloading:  71%|███████   | 355M/501M [00:50<00:20, 7.27MB/s]#015Downloading:  71%|███████   | 356M/501M [00:50<00:19, 7.27MB/s]#015Downloading:  71%|███████▏  | 358M/501M [00:50<00:19, 7.26MB/s]#015Downloading:  72%|███████▏  | 359M/501M [00:50<00:19, 7.26MB/s]#015Downloading:  72%|███████▏  | 361M/501M [00:50<00:19, 7.26MB/s]#015Downloading:  72%|███████▏  | 363M/501M [00:51<00:19, 7.26MB/s]#015Downloading:  73%|███████▎  | 364M/501M [00:51<00:18, 7.28MB/s]#015Downloading:  73%|███████▎  | 366M/501M [00:51<00:18, 7.27MB/s]#015Downloading:  73%|███████▎  | 367M/501M [00:51<00:18, 7.27MB/s]#015Downloading:  74%|███████▎  | 369M/501M [00:52<00:18, 7.26MB/s]#015Downloading:  74%|███████▍  | 370M/501M [00:52<00:18, 7.26MB/s]#015Downloading:  74%|███████▍  | 372M/501M [00:52<00:17, 7.26MB/s]#015Downloading:  75%|███████▍  | 374M/501M [00:52<00:17, 7.26MB/s]#015Downloading:  75%|███████▍  | 375M/501M [00:52<00:17, 7.26MB/s]#015Downloading:  75%|███████▌  | 377M/501M [00:53<00:17, 7.28MB/s]#015Downloading:  75%|███████▌  | 378M/501M [00:53<00:16, 7.27MB/s]#015Downloading:  76%|███████▌  | 380M/501M [00:53<00:16, 7.27MB/s]#015Downloading:  76%|███████▌  | 381M/501M [00:53<00:16, 7.26MB/s]#015Downloading:  76%|███████▋  | 383M/501M [00:53<00:16, 7.26MB/s]#015Downloading:  77%|███████▋  | 385M/501M [00:54<00:16, 7.26MB/s]#015Downloading:  77%|███████▋  | 386M/501M [00:54<00:15, 7.26MB/s]#015Downloading:  77%|███████▋  | 388M/501M [00:54<00:15, 7.26MB/s]#015Downloading:  78%|███████▊  | 389M/501M [00:54<00:15, 7.28MB/s]#015Downloading:  78%|███████▊  | 391M/501M [00:55<00:15, 7.27MB/s]#015Downloading:  78%|███████▊  | 393M/501M [00:55<00:14, 7.27MB/s]#015Downloading:  79%|███████▊  | 394M/501M [00:55<00:14, 7.26MB/s]#015Downloading:  79%|███████▉  | 396M/501M [00:55<00:14, 7.26MB/s]#015Downloading:  79%|███████▉  | 397M/501M [00:55<00:14, 7.26MB/s]#015Downloading:  80%|███████▉  | 399M/501M [00:56<00:14, 7.26MB/s]#015Downloading:  80%|███████▉  | 400M/501M [00:56<00:13, 7.28MB/s]#015Downloading:  80%|████████  | 402M/501M [00:56<00:13, 7.27MB/s]#015Downloading:  81%|████████  | 404M/501M [00:56<00:13, 7.27MB/s]#015Downloading:  81%|████████  | 405M/501M [00:57<00:13, 7.26MB/s]#015Downloading:  81%|████████  | 407M/501M [00:57<00:13, 7.26MB/s]#015Downloading:  81%|████████▏ | 408M/501M [00:57<00:12, 7.26MB/s]#015Downloading:  82%|████████▏ | 410M/501M [00:57<00:12, 7.26MB/s]#015Downloading:  82%|████████▏ | 411M/501M [00:57<00:11, 8.12MB/s]#015Downloading:  82%|████████▏ | 412M/501M [00:57<00:11, 8.01MB/s]#015Downloading:  82%|████████▏ | 413M/501M [00:58<00:13, 6.43MB/s]#015Downloading:  83%|████████▎ | 415M/501M [00:58<00:13, 6.58MB/s]#015Downloading:  83%|████████▎ | 416M/501M [00:58<00:12, 6.77MB/s]#015Downloading:  83%|████████▎ | 418M/501M [00:58<00:12, 6.91MB/s]#015Downloading:  84%|████████▎ | 419M/501M [00:58<00:11, 7.01MB/s]#015Downloading:  84%|████████▍ | 421M/501M [00:59<00:11, 7.08MB/s]#015Downloading:  84%|████████▍ | 422M/501M [00:59<00:11, 7.13MB/s]#015Downloading:  85%|████████▍ | 424M/501M [00:59<00:10, 7.17MB/s]#015Downloading:  85%|████████▍ | 426M/501M [00:59<00:10, 7.19MB/s]#015Downloading:  85%|████████▌ | 427M/501M [01:00<00:10, 7.23MB/s]#015Downloading:  86%|████████▌ | 429M/501M [01:00<00:10, 7.24MB/s]#015Downloading:  86%|████████▌ | 430M/501M [01:00<00:09, 7.24MB/s]#015Downloading:  86%|████████▌ | 432M/501M [01:00<00:09, 7.25MB/s]#015Downloading:  86%|████████▋ | 433M/501M [01:00<00:09, 7.24MB/s]#015Downloading:  87%|████████▋ | 435M/501M [01:01<00:09, 7.25MB/s]#015Downloading:  87%|████████▋ | 437M/501M [01:01<00:08, 7.25MB/s]#015Downloading:  87%|████████▋ | 438M/501M [01:01<00:08, 7.25MB/s]#015Downloading:  88%|████████▊ | 440M/501M [01:01<00:08, 7.26MB/s]#015Downloading:  88%|████████▊ | 441M/501M [01:02<00:08, 7.28MB/s]#015Downloading:  88%|████████▊ | 443M/501M [01:02<00:08, 7.27MB/s]#015Downloading:  89%|████████▊ | 444M/501M [01:02<00:07, 7.27MB/s]#015Downloading:  89%|████████▉ | 446M/501M [01:02<00:07, 7.26MB/s]#015Downloading:  89%|████████▉ | 448M/501M [01:02<00:07, 7.26MB/s]#015Downloading:  90%|████████▉ | 449M/501M [01:03<00:07, 7.26MB/s]#015Downloading:  90%|████████▉ | 451M/501M [01:03<00:06, 7.26MB/s]#015Downloading:  90%|█████████ | 452M/501M [01:03<00:06, 7.61MB/s]#015Downloading:  91%|█████████ | 454M/501M [01:03<00:05, 8.53MB/s]#015Downloading:  91%|█████████ | 455M/501M [01:03<00:06, 7.51MB/s]#015Downloading:  91%|█████████ | 456M/501M [01:03<00:07, 6.15MB/s]#015Downloading:  91%|█████████ | 457M/501M [01:04<00:06, 6.35MB/s]#015Downloading:  92%|█████████▏| 459M/501M [01:04<00:06, 6.59MB/s]#015Downloading:  92%|█████████▏| 460M/501M [01:04<00:06, 6.78MB/s]#015Downloading:  92%|█████████▏| 462M/501M [01:04<00:05, 6.92MB/s]#015Downloading:  92%|█████████▏| 463M/501M [01:05<00:05, 7.01MB/s]#015Downloading:  93%|█████████▎| 465M/501M [01:05<00:05, 7.11MB/s]#015Downloading:  93%|█████████▎| 467M/501M [01:05<00:04, 7.15MB/s]#015Downloading:  93%|█████████▎| 468M/501M [01:05<00:04, 7.18MB/s]#015Downloading:  94%|█████████▎| 470M/501M [01:05<00:04, 7.20MB/s]#015Downloading:  94%|█████████▍| 471M/501M [01:06<00:04, 7.22MB/s]#015Downloading:  94%|█████████▍| 473M/501M [01:06<00:03, 7.23MB/s]#015Downloading:  95%|█████████▍| 474M/501M [01:06<00:03, 7.24MB/s]#015Downloading:  95%|█████████▍| 476M/501M [01:06<00:03, 7.24MB/s]#015Downloading:  95%|█████████▌| 478M/501M [01:06<00:03, 7.27MB/s]#015Downloading:  96%|█████████▌| 479M/501M [01:07<00:03, 7.26MB/s]#015Downloading:  96%|█████████▌| 481M/501M [01:07<00:02, 7.26MB/s]#015Downloading:  96%|█████████▌| 482M/501M [01:07<00:02, 7.26MB/s]#015Downloading:  97%|█████████▋| 484M/501M [01:07<00:02, 7.26MB/s]#015Downloading:  97%|█████████▋| 485M/501M [01:08<00:02, 7.26MB/s]#015Downloading:  97%|█████████▋| 487M/501M [01:08<00:01, 7.26MB/s]#015Downloading:  97%|█████████▋| 489M/501M [01:08<00:01, 7.26MB/s]#015Downloading:  98%|█████████▊| 490M/501M [01:08<00:01, 7.28MB/s]#015Downloading:  98%|█████████▊| 492M/501M [01:08<00:01, 7.27MB/s]#015Downloading:  98%|█████████▊| 493M/501M [01:09<00:01, 7.27MB/s]#015Downloading:  99%|█████████▊| 495M/501M [01:09<00:00, 7.26MB/s]#015Downloading:  99%|█████████▉| 496M/501M [01:09<00:00, 7.26MB/s]#015Downloading:  99%|█████████▉| 498M/501M [01:09<00:00, 7.26MB/s]#015Downloading: 100%|█████████▉| 500M/501M [01:10<00:00, 7.26MB/s]#015Downloading: 100%|█████████▉| 501M/501M [01:10<00:00, 7.28MB/s]#015Downloading: 100%|██████████| 501M/501M [01:10<00:00, 7.13MB/s]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:28 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:28 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:28 - INFO - filelock -   Lock 139662332854000 released on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:28 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - transformers.modeling_utils -   Weights of RobertaForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/opt/ml/input/data/train', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_predict=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, labels='/opt/ml/input/data/label/label.txt', learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=0, no_cuda=False, num_train_epochs=5.0, output_dir='/opt/ml/model', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=500, seed=42, server_ip='', server_port='', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -   Creating features from dataset file at /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   Writing example 0 of 1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   guid: train-1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   tokens: <s> It 's the thought that count s , but our Online Returns Center makes g ift ex changes and return s simple ( just in case ! ) https : ( link : https :// am zn . to / 2 l 6 q Y K G ) am zn . to / 2 l 6 q Y K G Did you know you can trade in select Apple , Samsung , and other table ts ? With the Amazon Trade - in program , you can re ceive an Amazon G ift Card + 25 % off t oward a new Fire table t when you trade in your used table ts . ( link : https :// am zn . to / 2 </s> </s>\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   input_ids: 0 243 18 627 26086 6025 11432 29 6 4297 2126 19449 48826 25636 39082 571 9417 3463 34980 463 30921 29 41918 1640 8987 179 11173 328 43 13082 35 1640 12139 35 13082 640 424 30083 4 560 73 176 462 401 1343 975 530 534 43 424 30083 4 560 73 176 462 401 1343 975 530 534 20328 6968 27066 6968 7424 18582 179 38450 20770 6 35932 6 463 7443 14595 1872 116 3908 627 25146 35996 12 179 28644 6 6968 7424 241 6550 260 25146 534 9417 23818 2744 1244 207 1529 90 36750 102 4651 17260 14595 90 14746 6968 18582 179 16625 6199 14595 1872 4 1640 12139 35 13082 640 424 30083 4 560 73 176 2 2\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 6 -100 6 6 6 0 3 3 6 6 -100 6 -100 6 6 -100 6 6 6 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 6 6 6 6 6 6 6 0 6 0 6 6 6 6 -100 6 6 6 0 6 6 6 6 6 6 6 6 -100 6 0 6 -100 6 6 6 6 6 6 -100 6 6 0 6 -100 6 6 6 6 6 6 6 -100 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -   Saving features into cached file /opt/ml/input/data/train/cached_train_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -   ***** Running training *****\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Num examples = 1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Num Epochs = 5\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Instantaneous batch size per GPU = 8\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Total optimization steps = 5\u001b[0m\n",
      "\u001b[34m#015Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|██████████| 1/1 [00:03<00:00,  3.47s/it]#033[A#015Iteration: 100%|██████████| 1/1 [00:03<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  20%|██        | 1/5 [00:03<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|██████████| 1/1 [00:02<00:00,  2.91s/it]#033[A#015Iteration: 100%|██████████| 1/1 [00:02<00:00,  2.91s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  40%|████      | 2/5 [00:06<00:09,  3.30s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]#033[A#015Iteration: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  60%|██████    | 3/5 [00:09<00:06,  3.15s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]#033[A#015Iteration: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  80%|████████  | 4/5 [00:11<00:03,  3.04s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it]#033[A#015Iteration: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch: 100%|██████████| 5/5 [00:14<00:00,  2.97s/it]#015Epoch: 100%|██████████| 5/5 [00:14<00:00,  2.95s/it]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - run_ner -    global_step = 5, average loss = 1.5162001132965088\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - run_ner -   Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.configuration_utils -   Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.modeling_utils -   Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   Model name '/opt/ml/model' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/opt/ml/model' is a path, a model identifier, or url to a directory containing tokenizer files.\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   Didn't find file /opt/ml/model/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/vocab.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/merges.txt\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file None\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - run_ner -   Evaluate the following checkpoints: ['/opt/ml/model']\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.configuration_utils -   loading configuration file /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.modeling_utils -   loading weights file /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -   Creating features from dataset file at /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -   Saving features into cached file /opt/ml/input/data/train/cached_dev_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -   ***** Running evaluation  *****\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -     Num examples = 0\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -     Batch size = 8\u001b[0m\n",
      "\u001b[34m#015Evaluating: 0it [00:00, ?it/s]#015Evaluating: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"transformers-train.py\", line 56, in <module>\n",
      "    run_ner.main()\n",
      "  File \"/opt/ml/code/run_ner.py\", line 649, in main\n",
      "    result, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=\"dev\", prefix=global_step)\n",
      "  File \"/opt/ml/code/run_ner.py\", line 312, in evaluate\n",
      "    eval_loss = eval_loss / nb_eval_steps\u001b[0m\n",
      "\u001b[34mZeroDivisionError: float division by zero\u001b[0m\n",
      "\u001b[34m2020-02-21 10:46:52,159 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python transformers-train.py --model_name_or_path roberta-base --model_type roberta --num_train_epochs 5.0\"\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:08 - WARNING - run_ner -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:09 - INFO - filelock -   Lock 139662613216728 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:09 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpvm02khev\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/524 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 524/524 [00:00<00:00, 458kB/s]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - filelock -   Lock 139662613216728 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:10 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:11 - INFO - filelock -   Lock 139662613216728 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:11 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpunubg3ci\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]#015Downloading:   2%|â         | 17.4k/899k [00:00<00:10, 83.0kB/s]#015Downloading:   6%|â         | 52.2k/899k [00:00<00:08, 97.5kB/s]#015Downloading:  14%|ââ        | 122k/899k [00:00<00:06, 124kB/s]  #015Downloading:  29%|âââ       | 261k/899k [00:00<00:03, 164kB/s]#015Downloading:  64%|âââââââ   | 573k/899k [00:01<00:01, 223kB/s]#015Downloading: 100%|ââââââââââ| 899k/899k [00:01<00:00, 849kB/s]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:13 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:13 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:13 - INFO - filelock -   Lock 139662613216728 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:14 - INFO - filelock -   Lock 139662332852600 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:14 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpc68i9ccp\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]#015Downloading:   4%|â         | 17.4k/456k [00:00<00:05, 82.3kB/s]#015Downloading:  11%|ââ        | 52.2k/456k [00:00<00:04, 97.0kB/s]#015Downloading:  27%|âââ       | 122k/456k [00:00<00:02, 123kB/s]  #015Downloading:  57%|ââââââ    | 261k/456k [00:00<00:01, 163kB/s]#015Downloading: 100%|ââââââââââ| 456k/456k [00:00<00:00, 538kB/s]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - filelock -   Lock 139662332852600 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:16 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:17 - INFO - filelock -   Lock 139662332854000 acquired on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:45:17 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpdb7xyv5y\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]#015Downloading:   0%|          | 1.02k/501M [00:00<29:21:04, 4.74kB/s]#015Downloading:   0%|          | 34.8k/501M [00:00<20:48:44, 6.69kB/s]#015Downloading:   0%|          | 104k/501M [00:00<14:41:48, 9.47kB/s] #015Downloading:   0%|          | 225k/501M [00:00<10:21:36, 13.4kB/s]#015Downloading:   0%|          | 469k/501M [00:01<7:17:08, 19.1kB/s] #015Downloading:   0%|          | 957k/501M [00:01<5:06:48, 27.2kB/s]#015Downloading:   0%|          | 1.93M/501M [00:01<3:34:54, 38.7kB/s]#015Downloading:   1%|          | 3.51M/501M [00:01<2:30:18, 55.2kB/s]#015Downloading:   1%|          | 5.08M/501M [00:01<1:45:13, 78.6kB/s]#015Downloading:   1%|â         | 6.65M/501M [00:02<1:13:45, 112kB/s] #015Downloading:   2%|â         | 8.22M/501M [00:02<51:48, 159kB/s]  #015Downloading:   2%|â         | 9.80M/501M [00:02<36:29, 224kB/s]#015Downloading:   2%|â         | 11.4M/501M [00:02<25:47, 316kB/s]#015Downloading:   3%|â         | 12.9M/501M [00:03<18:20, 444kB/s]#015Downloading:   3%|â         | 14.5M/501M [00:03<13:07, 618kB/s]#015Downloading:   3%|â         | 16.1M/501M [00:03<09:29, 852kB/s]#015Downloading:   4%|â         | 17.7M/501M [00:03<06:57, 1.16MB/s]#015Downloading:   4%|â         | 19.3M/501M [00:03<05:11, 1.55MB/s]#015Downloading:   4%|â         | 20.8M/501M [00:04<03:56, 2.03MB/s]#015Downloading:   4%|â         | 22.4M/501M [00:04<03:05, 2.59MB/s]#015Downloading:   5%|â         | 24.0M/501M [00:04<02:28, 3.20MB/s]#015Downloading:   5%|â         | 25.5M/501M [00:04<02:03, 3.85MB/s]#015Downloading:   5%|â         | 27.1M/501M [00:04<01:45, 4.49MB/s]#015Downloading:   6%|â         | 28.7M/501M [00:05<01:33, 5.07MB/s]#015Downloading:   6%|â         | 30.3M/501M [00:05<01:24, 5.57MB/s]#015Downloading:   6%|â         | 31.9M/501M [00:05<01:18, 5.99MB/s]#015Downloading:   7%|â         | 33.4M/501M [00:05<01:13, 6.32MB/s]#015Downloading:   7%|â         | 35.0M/501M [00:06<01:10, 6.58MB/s]#015Downloading:   7%|â         | 36.6M/501M [00:06<01:08, 6.77MB/s]#015Downloading:   8%|â         | 38.1M/501M [00:06<01:07, 6.90MB/s]#015Downloading:   8%|â         | 39.7M/501M [00:06<01:05, 7.01MB/s]#015Downloading:   8%|â         | 41.3M/501M [00:06<01:04, 7.10MB/s]#015Downloading:   9%|â         | 42.9M/501M [00:07<01:04, 7.15MB/s]#015Downloading:   9%|â         | 44.4M/501M [00:07<01:03, 7.18MB/s]#015Downloading:   9%|â         | 46.0M/501M [00:07<01:03, 7.20MB/s]#015Downloading:   9%|â         | 47.6M/501M [00:07<01:02, 7.22MB/s]#015Downloading:  10%|â         | 49.2M/501M [00:08<01:02, 7.23MB/s]#015Downloading:  10%|â         | 50.7M/501M [00:08<01:02, 7.24MB/s]#015Downloading:  10%|â         | 52.3M/501M [00:08<01:01, 7.24MB/s]#015Downloading:  11%|â         | 53.9M/501M [00:08<01:01, 7.27MB/s]#015Downloading:  11%|â         | 55.5M/501M [00:08<01:01, 7.26MB/s]#015Downloading:  11%|ââ        | 57.0M/501M [00:09<01:01, 7.26MB/s]#015Downloading:  12%|ââ        | 58.6M/501M [00:09<01:00, 7.26MB/s]#015Downloading:  12%|ââ        | 60.2M/501M [00:09<01:00, 7.26MB/s]#015Downloading:  12%|ââ        | 61.8M/501M [00:09<01:00, 7.26MB/s]#015Downloading:  13%|ââ        | 63.3M/501M [00:09<01:00, 7.26MB/s]#015Downloading:  13%|ââ        | 64.9M/501M [00:10<00:59, 7.28MB/s]#015Downloading:  13%|ââ        | 66.5M/501M [00:10<00:59, 7.27MB/s]#015Downloading:  14%|ââ        | 68.1M/501M [00:10<00:59, 7.27MB/s]#015Downloading:  14%|ââ        | 69.6M/501M [00:10<00:59, 7.26MB/s]#015Downloading:  14%|ââ        | 71.2M/501M [00:11<00:59, 7.26MB/s]#015Downloading:  15%|ââ        | 72.8M/501M [00:11<00:59, 7.26MB/s]#015Downloading:  15%|ââ        | 74.4M/501M [00:11<00:58, 7.26MB/s]#015Downloading:  15%|ââ        | 76.0M/501M [00:11<00:58, 7.28MB/s]#015Downloading:  15%|ââ        | 77.5M/501M [00:11<00:58, 7.27MB/s]#015Downloading:  16%|ââ        | 79.1M/501M [00:12<00:58, 7.27MB/s]#015Downloading:  16%|ââ        | 80.7M/501M [00:12<00:57, 7.26MB/s]#015Downloading:  16%|ââ        | 82.2M/501M [00:12<00:57, 7.26MB/s]#015Downloading:  17%|ââ        | 83.8M/501M [00:12<00:57, 7.26MB/s]#015Downloading:  17%|ââ        | 85.4M/501M [00:13<00:57, 7.26MB/s]#015Downloading:  17%|ââ        | 87.0M/501M [00:13<00:57, 7.26MB/s]#015Downloading:  18%|ââ        | 88.5M/501M [00:13<00:47, 8.63MB/s]#015Downloading:  18%|ââ        | 89.5M/501M [00:13<00:50, 8.22MB/s]#015Downloading:  18%|ââ        | 90.4M/501M [00:13<01:03, 6.51MB/s]#015Downloading:  18%|ââ        | 91.7M/501M [00:13<01:04, 6.38MB/s]#015Downloading:  19%|ââ        | 93.3M/501M [00:14<01:01, 6.62MB/s]#015Downloading:  19%|ââ        | 94.8M/501M [00:14<00:59, 6.80MB/s]#015Downloading:  19%|ââ        | 96.4M/501M [00:14<00:58, 6.93MB/s]#015Downloading:  20%|ââ        | 98.0M/501M [00:14<00:57, 7.02MB/s]#015Downloading:  20%|ââ        | 99.6M/501M [00:14<00:56, 7.09MB/s]#015Downloading:  20%|ââ        | 101M/501M [00:15<00:55, 7.16MB/s] #015Downloading:  20%|ââ        | 103M/501M [00:15<00:55, 7.19MB/s]#015Downloading:  21%|ââ        | 104M/501M [00:15<00:55, 7.21MB/s]#015Downloading:  21%|ââ        | 106M/501M [00:15<00:54, 7.22MB/s]#015Downloading:  21%|âââ       | 107M/501M [00:16<00:54, 7.23MB/s]#015Downloading:  22%|âââ       | 109M/501M [00:16<00:54, 7.24MB/s]#015Downloading:  22%|âââ       | 111M/501M [00:16<00:53, 7.24MB/s]#015Downloading:  22%|âââ       | 112M/501M [00:16<00:53, 7.25MB/s]#015Downloading:  23%|âââ       | 114M/501M [00:16<00:53, 7.27MB/s]#015Downloading:  23%|âââ       | 115M/501M [00:17<00:53, 7.27MB/s]#015Downloading:  23%|âââ       | 117M/501M [00:17<00:52, 7.26MB/s]#015Downloading:  24%|âââ       | 118M/501M [00:17<00:52, 7.26MB/s]#015Downloading:  24%|âââ       | 120M/501M [00:17<00:52, 7.26MB/s]#015Downloading:  24%|âââ       | 122M/501M [00:17<00:52, 7.26MB/s]#015Downloading:  25%|âââ       | 123M/501M [00:18<00:52, 7.26MB/s]#015Downloading:  25%|âââ       | 125M/501M [00:18<00:51, 7.26MB/s]#015Downloading:  25%|âââ       | 126M/501M [00:18<00:51, 7.26MB/s]#015Downloading:  26%|âââ       | 128M/501M [00:18<00:51, 7.26MB/s]#015Downloading:  26%|âââ       | 129M/501M [00:19<00:51, 7.28MB/s]#015Downloading:  26%|âââ       | 131M/501M [00:19<00:50, 7.27MB/s]#015Downloading:  26%|âââ       | 133M/501M [00:19<00:50, 7.27MB/s]#015Downloading:  27%|âââ       | 134M/501M [00:19<00:50, 7.26MB/s]#015Downloading:  27%|âââ       | 136M/501M [00:19<00:50, 7.26MB/s]#015Downloading:  27%|âââ       | 137M/501M [00:20<00:50, 7.26MB/s]#015Downloading:  28%|âââ       | 139M/501M [00:20<00:49, 7.26MB/s]#015Downloading:  28%|âââ       | 141M/501M [00:20<00:49, 7.26MB/s]#015Downloading:  28%|âââ       | 142M/501M [00:20<00:49, 7.28MB/s]#015Downloading:  29%|âââ       | 144M/501M [00:21<00:49, 7.27MB/s]#015Downloading:  29%|âââ       | 145M/501M [00:21<00:48, 7.27MB/s]#015Downloading:  29%|âââ       | 147M/501M [00:21<00:48, 7.26MB/s]#015Downloading:  30%|âââ       | 148M/501M [00:21<00:48, 7.26MB/s]#015Downloading:  30%|âââ       | 150M/501M [00:21<00:48, 7.26MB/s]#015Downloading:  30%|âââ       | 152M/501M [00:22<00:48, 7.26MB/s]#015Downloading:  31%|âââ       | 153M/501M [00:22<00:47, 7.28MB/s]#015Downloading:  31%|âââ       | 155M/501M [00:22<00:47, 7.27MB/s]#015Downloading:  31%|âââ       | 156M/501M [00:22<00:47, 7.27MB/s]#015Downloading:  31%|ââââ      | 158M/501M [00:22<00:47, 7.26MB/s]#015Downloading:  32%|ââââ      | 159M/501M [00:23<00:47, 7.26MB/s]#015Downloading:  32%|ââââ      | 161M/501M [00:23<00:46, 7.26MB/s]#015Downloading:  32%|ââââ      | 163M/501M [00:23<00:46, 7.26MB/s]#015Downloading:  33%|ââââ      | 164M/501M [00:23<00:46, 7.26MB/s]#015Downloading:  33%|ââââ      | 166M/501M [00:24<00:46, 7.28MB/s]#015Downloading:  33%|ââââ      | 167M/501M [00:24<00:45, 7.27MB/s]#015Downloading:  34%|ââââ      | 169M/501M [00:24<00:45, 7.27MB/s]#015Downloading:  34%|ââââ      | 170M/501M [00:24<00:45, 7.26MB/s]#015Downloading:  34%|ââââ      | 172M/501M [00:24<00:45, 7.26MB/s]#015Downloading:  35%|ââââ      | 174M/501M [00:25<00:45, 7.26MB/s]#015Downloading:  35%|ââââ      | 175M/501M [00:25<00:44, 7.26MB/s]#015Downloading:  35%|ââââ      | 177M/501M [00:25<00:44, 7.26MB/s]#015Downloading:  36%|ââââ      | 178M/501M [00:25<00:44, 7.26MB/s]#015Downloading:  36%|ââââ      | 180M/501M [00:26<00:44, 7.28MB/s]#015Downloading:  36%|ââââ      | 181M/501M [00:26<00:43, 7.27MB/s]#015Downloading:  37%|ââââ      | 183M/501M [00:26<00:43, 7.27MB/s]#015Downloading:  37%|ââââ      | 185M/501M [00:26<00:43, 7.26MB/s]#015Downloading:  37%|ââââ      | 186M/501M [00:26<00:43, 7.26MB/s]#015Downloading:  37%|ââââ      | 188M/501M [00:27<00:43, 7.26MB/s]#015Downloading:  38%|ââââ      | 189M/501M [00:27<00:42, 7.26MB/s]#015Downloading:  38%|ââââ      | 191M/501M [00:27<00:42, 7.26MB/s]#015Downloading:  38%|ââââ      | 192M/501M [00:27<00:42, 7.28MB/s]#015Downloading:  39%|ââââ      | 194M/501M [00:27<00:42, 7.27MB/s]#015Downloading:  39%|ââââ      | 196M/501M [00:28<00:42, 7.27MB/s]#015Downloading:  39%|ââââ      | 197M/501M [00:28<00:41, 7.26MB/s]#015Downloading:  40%|ââââ      | 199M/501M [00:28<00:41, 7.26MB/s]#015Downloading:  40%|ââââ      | 200M/501M [00:28<00:41, 7.26MB/s]#015Downloading:  40%|ââââ      | 202M/501M [00:29<00:41, 7.26MB/s]#015Downloading:  41%|ââââ      | 204M/501M [00:29<00:40, 7.28MB/s]#015Downloading:  41%|ââââ      | 205M/501M [00:29<00:40, 7.27MB/s]#015Downloading:  41%|ââââ      | 207M/501M [00:29<00:40, 7.27MB/s]#015Downloading:  42%|âââââ     | 208M/501M [00:29<00:40, 7.26MB/s]#015Downloading:  42%|âââââ     | 210M/501M [00:30<00:40, 7.26MB/s]#015Downloading:  42%|âââââ     | 211M/501M [00:30<00:39, 7.26MB/s]#015Downloading:  42%|âââââ     | 213M/501M [00:30<00:39, 7.26MB/s]#015Downloading:  43%|âââââ     | 215M/501M [00:30<00:39, 7.26MB/s]#015Downloading:  43%|âââââ     | 216M/501M [00:31<00:39, 7.26MB/s]#015Downloading:  43%|âââââ     | 218M/501M [00:31<00:38, 7.28MB/s]#015Downloading:  44%|âââââ     | 219M/501M [00:31<00:38, 7.27MB/s]#015Downloading:  44%|âââââ     | 221M/501M [00:31<00:38, 7.27MB/s]#015Downloading:  44%|âââââ     | 222M/501M [00:31<00:38, 7.26MB/s]#015Downloading:  45%|âââââ     | 224M/501M [00:32<00:38, 7.26MB/s]#015Downloading:  45%|âââââ     | 226M/501M [00:32<00:37, 7.26MB/s]#015Downloading:  45%|âââââ     | 227M/501M [00:32<00:37, 7.26MB/s]#015Downloading:  46%|âââââ     | 229M/501M [00:32<00:37, 7.28MB/s]#015Downloading:  46%|âââââ     | 230M/501M [00:32<00:37, 7.27MB/s]#015Downloading:  46%|âââââ     | 232M/501M [00:33<00:37, 7.27MB/s]#015Downloading:  47%|âââââ     | 233M/501M [00:33<00:36, 7.26MB/s]#015Downloading:  47%|âââââ     | 235M/501M [00:33<00:36, 7.26MB/s]#015Downloading:  47%|âââââ     | 237M/501M [00:33<00:36, 7.26MB/s]#015Downloading:  48%|âââââ     | 238M/501M [00:34<00:36, 7.26MB/s]#015Downloading:  48%|âââââ     | 240M/501M [00:34<00:35, 7.28MB/s]#015Downloading:  48%|âââââ     | 241M/501M [00:34<00:35, 7.26MB/s]#015Downloading:  48%|âââââ     | 243M/501M [00:34<00:35, 7.27MB/s]#015Downloading:  49%|âââââ     | 244M/501M [00:34<00:35, 7.27MB/s]#015Downloading:  49%|âââââ     | 246M/501M [00:35<00:35, 7.26MB/s]#015Downloading:  49%|âââââ     | 248M/501M [00:35<00:34, 7.26MB/s]#015Downloading:  50%|âââââ     | 249M/501M [00:35<00:34, 7.26MB/s]#015Downloading:  50%|âââââ     | 251M/501M [00:35<00:34, 7.26MB/s]#015Downloading:  50%|âââââ     | 252M/501M [00:35<00:34, 7.28MB/s]#015Downloading:  51%|âââââ     | 254M/501M [00:36<00:34, 7.27MB/s]#015Downloading:  51%|âââââ     | 255M/501M [00:36<00:33, 7.27MB/s]#015Downloading:  51%|ââââââ    | 257M/501M [00:36<00:33, 7.26MB/s]#015Downloading:  52%|ââââââ    | 259M/501M [00:36<00:33, 7.26MB/s]#015Downloading:  52%|ââââââ    | 260M/501M [00:37<00:33, 7.26MB/s]#015Downloading:  52%|ââââââ    | 262M/501M [00:37<00:32, 7.26MB/s]#015Downloading:  53%|ââââââ    | 263M/501M [00:37<00:32, 7.26MB/s]#015Downloading:  53%|ââââââ    | 265M/501M [00:37<00:32, 7.28MB/s]#015Downloading:  53%|ââââââ    | 267M/501M [00:37<00:32, 7.27MB/s]#015Downloading:  53%|ââââââ    | 268M/501M [00:38<00:32, 7.27MB/s]#015Downloading:  54%|ââââââ    | 270M/501M [00:38<00:31, 7.26MB/s]#015Downloading:  54%|ââââââ    | 271M/501M [00:38<00:31, 7.26MB/s]#015Downloading:  54%|ââââââ    | 273M/501M [00:38<00:31, 7.26MB/s]#015Downloading:  55%|ââââââ    | 274M/501M [00:39<00:31, 7.25MB/s]#015Downloading:  55%|ââââââ    | 276M/501M [00:39<00:31, 7.25MB/s]#015Downloading:  55%|ââââââ    | 278M/501M [00:39<00:30, 7.28MB/s]#015Downloading:  56%|ââââââ    | 279M/501M [00:39<00:30, 7.27MB/s]#015Downloading:  56%|ââââââ    | 281M/501M [00:39<00:30, 7.27MB/s]#015Downloading:  56%|ââââââ    | 282M/501M [00:40<00:30, 7.26MB/s]#015Downloading:  57%|ââââââ    | 284M/501M [00:40<00:29, 7.26MB/s]#015Downloading:  57%|ââââââ    | 285M/501M [00:40<00:29, 7.26MB/s]#015Downloading:  57%|ââââââ    | 287M/501M [00:40<00:29, 7.25MB/s]#015Downloading:  58%|ââââââ    | 289M/501M [00:40<00:29, 7.28MB/s]#015Downloading:  58%|ââââââ    | 290M/501M [00:41<00:29, 7.27MB/s]#015Downloading:  58%|ââââââ    | 292M/501M [00:41<00:28, 7.27MB/s]#015Downloading:  59%|ââââââ    | 293M/501M [00:41<00:28, 7.26MB/s]#015Downloading:  59%|ââââââ    | 295M/501M [00:41<00:28, 7.26MB/s]#015Downloading:  59%|ââââââ    | 296M/501M [00:42<00:28, 7.26MB/s]#015Downloading:  59%|ââââââ    | 298M/501M [00:42<00:27, 7.26MB/s]#015Downloading:  60%|ââââââ    | 300M/501M [00:42<00:27, 7.27MB/s]#015Downloading:  60%|ââââââ    | 301M/501M [00:42<00:27, 7.27MB/s]#015Downloading:  60%|ââââââ    | 303M/501M [00:42<00:27, 7.27MB/s]#015Downloading:  61%|ââââââ    | 304M/501M [00:43<00:27, 7.26MB/s]#015Downloading:  61%|ââââââ    | 306M/501M [00:43<00:26, 7.26MB/s]#015Downloading:  61%|ââ�\u001b[0m\n",
      "\u001b[34m�âââââ   | 307M/501M [00:43<00:26, 7.26MB/s]#015Downloading:  62%|âââââââ   | 309M/501M [00:43<00:26, 7.26MB/s]#015Downloading:  62%|âââââââ   | 311M/501M [00:44<00:26, 7.26MB/s]#015Downloading:  62%|âââââââ   | 312M/501M [00:44<00:24, 7.62MB/s]#015Downloading:  63%|âââââââ   | 314M/501M [00:44<00:21, 8.53MB/s]#015Downloading:  63%|âââââââ   | 315M/501M [00:44<00:24, 7.56MB/s]#015Downloading:  63%|âââââââ   | 315M/501M [00:44<00:30, 6.12MB/s]#015Downloading:  63%|âââââââ   | 317M/501M [00:44<00:29, 6.33MB/s]#015Downloading:  64%|âââââââ   | 318M/501M [00:45<00:27, 6.58MB/s]#015Downloading:  64%|âââââââ   | 320M/501M [00:45<00:26, 6.77MB/s]#015Downloading:  64%|âââââââ   | 322M/501M [00:45<00:25, 6.91MB/s]#015Downloading:  64%|âââââââ   | 323M/501M [00:45<00:25, 7.01MB/s]#015Downloading:  65%|âââââââ   | 325M/501M [00:45<00:24, 7.08MB/s]#015Downloading:  65%|âââââââ   | 326M/501M [00:46<00:24, 7.15MB/s]#015Downloading:  65%|âââââââ   | 328M/501M [00:46<00:24, 7.18MB/s]#015Downloading:  66%|âââââââ   | 330M/501M [00:46<00:23, 7.20MB/s]#015Downloading:  66%|âââââââ   | 331M/501M [00:46<00:23, 7.22MB/s]#015Downloading:  66%|âââââââ   | 333M/501M [00:47<00:23, 7.23MB/s]#015Downloading:  67%|âââââââ   | 334M/501M [00:47<00:23, 7.24MB/s]#015Downloading:  67%|âââââââ   | 336M/501M [00:47<00:22, 7.24MB/s]#015Downloading:  67%|âââââââ   | 337M/501M [00:47<00:22, 7.25MB/s]#015Downloading:  68%|âââââââ   | 339M/501M [00:47<00:22, 7.27MB/s]#015Downloading:  68%|âââââââ   | 341M/501M [00:48<00:22, 7.27MB/s]#015Downloading:  68%|âââââââ   | 342M/501M [00:48<00:21, 7.26MB/s]#015Downloading:  69%|âââââââ   | 344M/501M [00:48<00:21, 7.26MB/s]#015Downloading:  69%|âââââââ   | 345M/501M [00:48<00:21, 7.26MB/s]#015Downloading:  69%|âââââââ   | 347M/501M [00:48<00:21, 7.26MB/s]#015Downloading:  70%|âââââââ   | 348M/501M [00:49<00:21, 7.26MB/s]#015Downloading:  70%|âââââââ   | 350M/501M [00:49<00:20, 7.26MB/s]#015Downloading:  70%|âââââââ   | 352M/501M [00:49<00:20, 7.26MB/s]#015Downloading:  70%|âââââââ   | 353M/501M [00:49<00:20, 7.28MB/s]#015Downloading:  71%|âââââââ   | 355M/501M [00:50<00:20, 7.27MB/s]#015Downloading:  71%|âââââââ   | 356M/501M [00:50<00:19, 7.27MB/s]#015Downloading:  71%|ââââââââ  | 358M/501M [00:50<00:19, 7.26MB/s]#015Downloading:  72%|ââââââââ  | 359M/501M [00:50<00:19, 7.26MB/s]#015Downloading:  72%|ââââââââ  | 361M/501M [00:50<00:19, 7.26MB/s]#015Downloading:  72%|ââââââââ  | 363M/501M [00:51<00:19, 7.26MB/s]#015Downloading:  73%|ââââââââ  | 364M/501M [00:51<00:18, 7.28MB/s]#015Downloading:  73%|ââââââââ  | 366M/501M [00:51<00:18, 7.27MB/s]#015Downloading:  73%|ââââââââ  | 367M/501M [00:51<00:18, 7.27MB/s]#015Downloading:  74%|ââââââââ  | 369M/501M [00:52<00:18, 7.26MB/s]#015Downloading:  74%|ââââââââ  | 370M/501M [00:52<00:18, 7.26MB/s]#015Downloading:  74%|ââââââââ  | 372M/501M [00:52<00:17, 7.26MB/s]#015Downloading:  75%|ââââââââ  | 374M/501M [00:52<00:17, 7.26MB/s]#015Downloading:  75%|ââââââââ  | 375M/501M [00:52<00:17, 7.26MB/s]#015Downloading:  75%|ââââââââ  | 377M/501M [00:53<00:17, 7.28MB/s]#015Downloading:  75%|ââââââââ  | 378M/501M [00:53<00:16, 7.27MB/s]#015Downloading:  76%|ââââââââ  | 380M/501M [00:53<00:16, 7.27MB/s]#015Downloading:  76%|ââââââââ  | 381M/501M [00:53<00:16, 7.26MB/s]#015Downloading:  76%|ââââââââ  | 383M/501M [00:53<00:16, 7.26MB/s]#015Downloading:  77%|ââââââââ  | 385M/501M [00:54<00:16, 7.26MB/s]#015Downloading:  77%|ââââââââ  | 386M/501M [00:54<00:15, 7.26MB/s]#015Downloading:  77%|ââââââââ  | 388M/501M [00:54<00:15, 7.26MB/s]#015Downloading:  78%|ââââââââ  | 389M/501M [00:54<00:15, 7.28MB/s]#015Downloading:  78%|ââââââââ  | 391M/501M [00:55<00:15, 7.27MB/s]#015Downloading:  78%|ââââââââ  | 393M/501M [00:55<00:14, 7.27MB/s]#015Downloading:  79%|ââââââââ  | 394M/501M [00:55<00:14, 7.26MB/s]#015Downloading:  79%|ââââââââ  | 396M/501M [00:55<00:14, 7.26MB/s]#015Downloading:  79%|ââââââââ  | 397M/501M [00:55<00:14, 7.26MB/s]#015Downloading:  80%|ââââââââ  | 399M/501M [00:56<00:14, 7.26MB/s]#015Downloading:  80%|ââââââââ  | 400M/501M [00:56<00:13, 7.28MB/s]#015Downloading:  80%|ââââââââ  | 402M/501M [00:56<00:13, 7.27MB/s]#015Downloading:  81%|ââââââââ  | 404M/501M [00:56<00:13, 7.27MB/s]#015Downloading:  81%|ââââââââ  | 405M/501M [00:57<00:13, 7.26MB/s]#015Downloading:  81%|ââââââââ  | 407M/501M [00:57<00:13, 7.26MB/s]#015Downloading:  81%|âââââââââ | 408M/501M [00:57<00:12, 7.26MB/s]#015Downloading:  82%|âââââââââ | 410M/501M [00:57<00:12, 7.26MB/s]#015Downloading:  82%|âââââââââ | 411M/501M [00:57<00:11, 8.12MB/s]#015Downloading:  82%|âââââââââ | 412M/501M [00:57<00:11, 8.01MB/s]#015Downloading:  82%|âââââââââ | 413M/501M [00:58<00:13, 6.43MB/s]#015Downloading:  83%|âââââââââ | 415M/501M [00:58<00:13, 6.58MB/s]#015Downloading:  83%|âââââââââ | 416M/501M [00:58<00:12, 6.77MB/s]#015Downloading:  83%|âââââââââ | 418M/501M [00:58<00:12, 6.91MB/s]#015Downloading:  84%|âââââââââ | 419M/501M [00:58<00:11, 7.01MB/s]#015Downloading:  84%|âââââââââ | 421M/501M [00:59<00:11, 7.08MB/s]#015Downloading:  84%|âââââââââ | 422M/501M [00:59<00:11, 7.13MB/s]#015Downloading:  85%|âââââââââ | 424M/501M [00:59<00:10, 7.17MB/s]#015Downloading:  85%|âââââââââ | 426M/501M [00:59<00:10, 7.19MB/s]#015Downloading:  85%|âââââââââ | 427M/501M [01:00<00:10, 7.23MB/s]#015Downloading:  86%|âââââââââ | 429M/501M [01:00<00:10, 7.24MB/s]#015Downloading:  86%|âââââââââ | 430M/501M [01:00<00:09, 7.24MB/s]#015Downloading:  86%|âââââââââ | 432M/501M [01:00<00:09, 7.25MB/s]#015Downloading:  86%|âââââââââ | 433M/501M [01:00<00:09, 7.24MB/s]#015Downloading:  87%|âââââââââ | 435M/501M [01:01<00:09, 7.25MB/s]#015Downloading:  87%|âââââââââ | 437M/501M [01:01<00:08, 7.25MB/s]#015Downloading:  87%|âââââââââ | 438M/501M [01:01<00:08, 7.25MB/s]#015Downloading:  88%|âââââââââ | 440M/501M [01:01<00:08, 7.26MB/s]#015Downloading:  88%|âââââââââ | 441M/501M [01:02<00:08, 7.28MB/s]#015Downloading:  88%|âââââââââ | 443M/501M [01:02<00:08, 7.27MB/s]#015Downloading:  89%|âââââââââ | 444M/501M [01:02<00:07, 7.27MB/s]#015Downloading:  89%|âââââââââ | 446M/501M [01:02<00:07, 7.26MB/s]#015Downloading:  89%|âââââââââ | 448M/501M [01:02<00:07, 7.26MB/s]#015Downloading:  90%|âââââââââ | 449M/501M [01:03<00:07, 7.26MB/s]#015Downloading:  90%|âââââââââ | 451M/501M [01:03<00:06, 7.26MB/s]#015Downloading:  90%|âââââââââ | 452M/501M [01:03<00:06, 7.61MB/s]#015Downloading:  91%|âââââââââ | 454M/501M [01:03<00:05, 8.53MB/s]#015Downloading:  91%|âââââââââ | 455M/501M [01:03<00:06, 7.51MB/s]#015Downloading:  91%|âââââââââ | 456M/501M [01:03<00:07, 6.15MB/s]#015Downloading:  91%|âââââââââ | 457M/501M [01:04<00:06, 6.35MB/s]#015Downloading:  92%|ââââââââââ| 459M/501M [01:04<00:06, 6.59MB/s]#015Downloading:  92%|ââââââââââ| 460M/501M [01:04<00:06, 6.78MB/s]#015Downloading:  92%|ââââââââââ| 462M/501M [01:04<00:05, 6.92MB/s]#015Downloading:  92%|ââââââââââ| 463M/501M [01:05<00:05, 7.01MB/s]#015Downloading:  93%|ââââââââââ| 465M/501M [01:05<00:05, 7.11MB/s]#015Downloading:  93%|ââââââââââ| 467M/501M [01:05<00:04, 7.15MB/s]#015Downloading:  93%|ââââââââââ| 468M/501M [01:05<00:04, 7.18MB/s]#015Downloading:  94%|ââââââââââ| 470M/501M [01:05<00:04, 7.20MB/s]#015Downloading:  94%|ââââââââââ| 471M/501M [01:06<00:04, 7.22MB/s]#015Downloading:  94%|ââââââââââ| 473M/501M [01:06<00:03, 7.23MB/s]#015Downloading:  95%|ââââââââââ| 474M/501M [01:06<00:03, 7.24MB/s]#015Downloading:  95%|ââââââââââ| 476M/501M [01:06<00:03, 7.24MB/s]#015Downloading:  95%|ââââââââââ| 478M/501M [01:06<00:03, 7.27MB/s]#015Downloading:  96%|ââââââââââ| 479M/501M [01:07<00:03, 7.26MB/s]#015Downloading:  96%|ââââââââââ| 481M/501M [01:07<00:02, 7.26MB/s]#015Downloading:  96%|ââââââââââ| 482M/501M [01:07<00:02, 7.26MB/s]#015Downloading:  97%|ââââââââââ| 484M/501M [01:07<00:02, 7.26MB/s]#015Downloading:  97%|ââââââââââ| 485M/501M [01:08<00:02, 7.26MB/s]#015Downloading:  97%|ââââââââââ| 487M/501M [01:08<00:01, 7.26MB/s]#015Downloading:  97%|ââââââââââ| 489M/501M [01:08<00:01, 7.26MB/s]#015Downloading:  98%|ââââââââââ| 490M/501M [01:08<00:01, 7.28MB/s]#015Downloading:  98%|ââââââââââ| 492M/501M [01:08<00:01, 7.27MB/s]#015Downloading:  98%|ââââââââââ| 493M/501M [01:09<00:01, 7.27MB/s]#015Downloading:  99%|ââââââââââ| 495M/501M [01:09<00:00, 7.26MB/s]#015Downloading:  99%|ââââââââââ| 496M/501M [01:09<00:00, 7.26MB/s]#015Downloading:  99%|ââââââââââ| 498M/501M [01:09<00:00, 7.26MB/s]#015Downloading: 100%|ââââââââââ| 500M/501M [01:10<00:00, 7.26MB/s]#015Downloading: 100%|ââââââââââ| 501M/501M [01:10<00:00, 7.28MB/s]#015Downloading: 100%|ââââââââââ| 501M/501M [01:10<00:00, 7.13MB/s]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:28 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:28 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:28 - INFO - filelock -   Lock 139662332854000 released on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:28 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - transformers.modeling_utils -   Weights of RobertaForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/opt/ml/input/data/train', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_predict=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, labels='/opt/ml/input/data/label/label.txt', learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=0, no_cuda=False, num_train_epochs=5.0, output_dir='/opt/ml/model', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=500, seed=42, server_ip='', server_port='', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -   Creating features from dataset file at /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   Writing example 0 of 1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   guid: train-1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   tokens: <s> It 's the thought that count s , but our Online Returns Center makes g ift ex changes and return s simple ( just in case ! ) https : ( link : https :// am zn . to / 2 l 6 q Y K G ) am zn . to / 2 l 6 q Y K G Did you know you can trade in select Apple , Samsung , and other table ts ? With the Amazon Trade - in program , you can re ceive an Amazon G ift Card + 25 % off t oward a new Fire table t when you trade in your used table ts . ( link : https :// am zn . to / 2 </s> </s>\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   input_ids: 0 243 18 627 26086 6025 11432 29 6 4297 2126 19449 48826 25636 39082 571 9417 3463 34980 463 30921 29 41918 1640 8987 179 11173 328 43 13082 35 1640 12139 35 13082 640 424 30083 4 560 73 176 462 401 1343 975 530 534 43 424 30083 4 560 73 176 462 401 1343 975 530 534 20328 6968 27066 6968 7424 18582 179 38450 20770 6 35932 6 463 7443 14595 1872 116 3908 627 25146 35996 12 179 28644 6 6968 7424 241 6550 260 25146 534 9417 23818 2744 1244 207 1529 90 36750 102 4651 17260 14595 90 14746 6968 18582 179 16625 6199 14595 1872 4 1640 12139 35 13082 640 424 30083 4 560 73 176 2 2\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 6 -100 6 6 6 0 3 3 6 6 -100 6 -100 6 6 -100 6 6 6 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 6 6 6 6 6 6 6 0 6 0 6 6 6 6 -100 6 6 6 0 6 6 6 6 6 6 6 6 -100 6 0 6 -100 6 6 6 6 6 6 -100 6 6 0 6 -100 6 6 6 6 6 6 6 -100 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -   Saving features into cached file /opt/ml/input/data/train/cached_train_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -   ***** Running training *****\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Num examples = 1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Num Epochs = 5\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Instantaneous batch size per GPU = 8\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:32 - INFO - run_ner -     Total optimization steps = 5\u001b[0m\n",
      "\u001b[34m#015Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|ââââââââââ| 1/1 [00:03<00:00,  3.47s/it]#033[A#015Iteration: 100%|ââââââââââ| 1/1 [00:03<00:00,  3.47s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  20%|ââ        | 1/5 [00:03<00:13,  3.47s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|ââââââââââ| 1/1 [00:02<00:00,  2.91s/it]#033[A#015Iteration: 100%|ââââââââââ| 1/1 [00:02<00:00,  2.91s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  40%|ââââ      | 2/5 [00:06<00:09,  3.30s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|ââââââââââ| 1/1 [00:02<00:00,  2.81s/it]#033[A#015Iteration: 100%|ââââââââââ| 1/1 [00:02<00:00,  2.81s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  60%|ââââââ    | 3/5 [00:09<00:06,  3.15s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|ââââââââââ| 1/1 [00:02<00:00,  2.76s/it]#033[A#015Iteration: 100%|ââââââââââ| 1/1 [00:02<00:00,  2.76s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  80%|ââââââââ  | 4/5 [00:11<00:03,  3.04s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0%|          | 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100%|ââââââââââ| 1/1 [00:02<00:00,  2.82s/it]#033[A#015Iteration: 100%|ââââââââââ| 1/1 [00:02<00:00,  2.82s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch: 100%|ââââââââââ| 5/5 [00:14<00:00,  2.97s/it]#015Epoch: 100%|ââââââââââ| 5/5 [00:14<00:00,  2.95s/it]\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - run_ner -    global_step = 5, average loss = 1.5162001132965088\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - run_ner -   Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.configuration_utils -   Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.modeling_utils -   Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   Model name '/opt/ml/model' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/opt/ml/model' is a path, a model identifier, or url to a directory containing tokenizer files.\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   Didn't find file /opt/ml/model/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/vocab.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/merges.txt\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file None\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - run_ner -   Evaluate the following checkpoints: ['/opt/ml/model']\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.configuration_utils -   loading configuration file /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:47 - INFO - transformers.modeling_utils -   loading weights file /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -   Creating features from dataset file at /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -   Saving features into cached file /opt/ml/input/data/train/cached_dev_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -   ***** Running evaluation  *****\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -     Num examples = 0\u001b[0m\n",
      "\u001b[34m02/21/2020 10:46:51 - INFO - run_ner -     Batch size = 8\u001b[0m\n",
      "\u001b[34m#015Evaluating: 0it [00:00, ?it/s]#015Evaluating: 0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"transformers-train.py\", line 56, in <module>\n",
      "    run_ner.main()\n",
      "  File \"/opt/ml/code/run_ner.py\", line 649, in main\n",
      "    result, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=\"dev\", prefix=global_step)\n",
      "  File \"/opt/ml/code/run_ner.py\", line 312, in evaluate\n",
      "    eval_loss = eval_loss / nb_eval_steps\u001b[0m\n",
      "\u001b[34mZeroDivisionError: float division by zero\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train': s3_input(train), 'dev': s3_input(dev), 'label': s3_input(label)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
