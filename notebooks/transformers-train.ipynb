{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:250%; font-weight:bold'>Train NER with huggingface/transformers</div>\n",
    "\n",
    "This notebook shows how to use `huggingface/transformers` on Amazon SageMaker to transfer-learn the Roberta language model into a new NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "import s3fs\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "from gtner_blog.util import split, bilou2bio, write_split, LabelCollector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download transformers NER scripts\n",
    "\n",
    "The `huggingface/transformers` repo contains two PyTorch scripts to download, namely `run_ner.py` and `utils_ner.py`. The following `bash` cell downloads version v2.5.0 which matches the library listed in `requirements.txt`.\n",
    "\n",
    "To minimize the dependencies installed to the training container, we also download the `seqeval` into `source_dir/` to emulate `pip install --no-deps seqeval`. The `seqeval.callbacks` depends on [Keras](https://github.com/chakki-works/seqeval/blob/v0.0.12/seqeval/callbacks.py) and [tensorflow](https://github.com/chakki-works/seqeval/blob/v0.0.12/requirements.txt), but `run_ner.py` does not uses these callbacks (and only `seqeval.metrics`), hence both dependencies can be [skipped](https://github.com/chakki-works/seqeval/blob/v0.0.12/seqeval/metrics/sequence_labeling.py).\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Note</summary>\n",
    "    <blockquote>As of this writing, the master branch of `huggingface/transformers` has relocated the NER scripts from `examples/` to `examples/ner/`, which is beyond the scope of this notebook.</blockquote>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ec2-user ec2-user    32 Feb 21 07:38 requirements.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 30349 Feb 21 16:06 \u001b[01;31m\u001b[Krun_ner.py\u001b[m\u001b[K\n",
      "drwxrwxr-x 3 ec2-user ec2-user  4096 Feb 21 08:12 \u001b[01;31m\u001b[Kseqeval\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  3111 Feb 21 16:06 \u001b[01;31m\u001b[Kseqeval/callbacks.py\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user     0 Feb 21 16:06 \u001b[01;31m\u001b[Kseqeval/__init__.py\u001b[m\u001b[K\n",
      "drwxrwxr-x 2 ec2-user ec2-user  4096 Feb 21 08:12 \u001b[01;31m\u001b[Kseqeval/metrics\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   371 Feb 21 16:06 \u001b[01;31m\u001b[Kseqeval/metrics/__init__.py\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 12604 Feb 21 16:06 \u001b[01;31m\u001b[Kseqeval/metrics/sequence_labeling.py\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  3559 Feb 21 16:04 transformers-train.py\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  8428 Feb 21 16:06 \u001b[01;31m\u001b[Kutils_ner.py\u001b[m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "GITHUB=https://raw.githubusercontent.com\n",
    "cd transformers-scripts\n",
    "\n",
    "# Download NER scripts\n",
    "for i in run_ner.py utils_ner.py\n",
    "do\n",
    "    curl --silent --location $GITHUB/huggingface/transformers/v2.5.0/examples/$i > $i\n",
    "done\n",
    "\n",
    "# Download seqeval\n",
    "mkdir -p seqeval/metrics\n",
    "for i in __init__.py callbacks.py metrics/__init__.py metrics/sequence_labeling.py\n",
    "do\n",
    "    curl --silent --location $GITHUB/chakki-works/seqeval/v0.0.12/seqeval/$i > seqeval/$i\n",
    "done\n",
    "\n",
    "ls -ald * seqeval/* seqeval/metrics/* | egrep --color=always 'run_ner.py|utils_ner.py|seqeval.*|^'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data channels\n",
    "\n",
    "Split the whole corpus in S3 into train:test = 3:1 proportion, then upload the splits to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/gt/test-gtner-blog-004/manifests/output/output.iob'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/transformers-data/train'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/transformers-data/dev'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/transformers-data/label'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket = 'gtner-blog'                # Change me as necessary\n",
    "gt_jobname = 'test-gtner-blog-004'   # Change me as necessary\n",
    "\n",
    "iob_file = f's3://{bucket}/gt/{gt_jobname}/manifests/output/output.iob'\n",
    "train = f's3://{bucket}/transformers-data/train'\n",
    "dev = f's3://{bucket}/transformers-data/dev'\n",
    "label = f's3://{bucket}/transformers-data/label'\n",
    "label_collector = LabelCollector()\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "with fs.open(iob_file, 'r') as f:\n",
    "    train_split = os.path.join(train, 'train.txt')\n",
    "    dev_split = os.path.join(dev, 'dev.txt')\n",
    "    \n",
    "    # Chain of functions: .iob -> bilou2bio -> label_collector -> split -> write_split.\n",
    "    write_split(split(label_collector(bilou2bio(f))), train_split, dev_split)\n",
    "\n",
    "with fs.open(os.path.join(label, 'label.txt'), 'w') as f:\n",
    "    for ner_tag in label_collector.sorted_labels:\n",
    "        f.write(f'{ner_tag}\\n')\n",
    "\n",
    "display(iob_file, train, dev, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training\n",
    "\n",
    "We create a PyTorch estimator with our entry point script `transformers-train.py`, a thin wrapper over `run_ner.py` that does the following:\n",
    "\n",
    "1. parse SageMaker's entry-point protocol, namely model and channel directories.\n",
    "2. pre-define a few arguments to `run-ner.py`: `{\"--do_train\", \"--do-eval\", \"--evaluate_during_train\", \"--data_dir\", \"--output_dir\", \"--label\"}`.\n",
    "3. passes the estimator's hyper-parameters as arguments to `run-ner.py`.\n",
    "   1. Each hyperparameter `abcd` will be passed down as `--abcd`.\n",
    "   2. The hyperparameters must not conflict with those in the above mentioned step 2.\n",
    "   3. The entry point only support `--abcd SOME_VALUE` form of arguments.\n",
    "\n",
    "```bash\n",
    "usage: run_ner.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\n",
    "                  --model_name_or_path MODEL_NAME_OR_PATH --output_dir\n",
    "                  OUTPUT_DIR [--labels LABELS] [--config_name CONFIG_NAME]\n",
    "                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
    "                  [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\n",
    "                  [--do_predict] [--evaluate_during_training]\n",
    "                  [--do_lower_case]\n",
    "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
    "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
    "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
    "                  [--learning_rate LEARNING_RATE]\n",
    "                  [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n",
    "                  [--max_grad_norm MAX_GRAD_NORM]\n",
    "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
    "                  [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n",
    "                  [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
    "                  [--eval_all_checkpoints] [--no_cuda]\n",
    "                  [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n",
    "                  [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n",
    "                  [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n",
    "                  [--server_port SERVER_PORT]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='transformers-train.py',\n",
    "                    source_dir='./transformers-scripts',\n",
    "                    role=get_execution_role(),\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m5.large',\n",
    "                    framework_version='1.3.1',\n",
    "                    py_version='py3',\n",
    "                    debugger_hook_config=False,\n",
    "                    hyperparameters={\n",
    "                        'num_train_epochs': 5.0,\n",
    "                        'model_type': 'roberta',\n",
    "                        'model_name_or_path': 'roberta-base'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-21 16:24:29 Starting - Starting the training job...\n",
      "2020-02-21 16:24:31 Starting - Launching requested ML instances......\n",
      "2020-02-21 16:25:34 Starting - Preparing the instances for training...\n",
      "2020-02-21 16:26:22 Downloading - Downloading input data...\n",
      "2020-02-21 16:26:53 Training - Downloading the training image...\n",
      "2020-02-21 16:27:24 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:24,991 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:24,996 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:25,008 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:25,009 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:25,224 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:25,224 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:25,224 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:25,225 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpor7aoq03/module_dir\u001b[0m\n",
      "\u001b[34mCollecting transformers==2.5.0\n",
      "  Downloading transformers-2.5.0-py3-none-any.whl (481 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\n",
      "  Downloading tensorboard-2.1.0-py3-none-any.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (1.11.7)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.2.20-cp36-cp36m-manylinux2010_x86_64.whl (690 kB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.5.0\n",
      "  Downloading tokenizers-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (4.36.1)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.11.2-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl (2.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (3.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (44.0.0.post20200106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (1.12.0)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (0.15.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (0.33.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.5.0->-r requirements.txt (line 1)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.5.0->-r requirements.txt (line 1)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.15.0,>=1.14.7 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.5.0->-r requirements.txt (line 1)) (1.14.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.5.0->-r requirements.txt (line 1)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.5.0->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (3.4.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.5.0->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.5.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, sacremoses, absl-py\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=35327 sha256=3a4ea3dc779bdde0b14b15c888a39fcf279e45fc8c96ac49f046a31981834287\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0gfc97ui/wheels/3a/0f/22/72253b78a2d686c3cd0fa38f4f3085755e6d9591200352a0d8\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=c40d5862962eff56ff7f9592ff66e90076c8bf98c12a935c8d705b503d30117d\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\n",
      "  Building wheel for absl-py (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=a44ffdfeaac2b929300934e4faa5d73651cc1ee523ca1253e8ef1d1aba4982cc\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name sacremoses absl-py\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, filelock, tokenizers, sacremoses, sentencepiece, transformers, pyasn1-modules, cachetools, google-auth, markdown, absl-py, grpcio, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-0.9.0 cachetools-4.0.0 default-user-module-name-1.0.0 filelock-3.0.12 google-auth-1.11.2 google-auth-oauthlib-0.4.1 grpcio-1.27.2 markdown-3.2.1 oauthlib-3.1.0 pyasn1-modules-0.2.8 regex-2020.2.20 requests-oauthlib-1.3.0 sacremoses-0.0.38 sentencepiece-0.1.85 tensorboard-2.1.0 tokenizers-0.5.0 transformers-2.5.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:33,124 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:33,137 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:33,149 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-21 16:27:33,160 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"dev\": \"/opt/ml/input/data/dev\",\n",
      "        \"label\": \"/opt/ml/input/data/label\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"num_train_epochs\": 5.0,\n",
      "        \"model_type\": \"roberta\",\n",
      "        \"model_name_or_path\": \"roberta-base\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"dev\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"label\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-02-21-16-24-29-699\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-21-16-24-29-699/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transformers-train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transformers-train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_name_or_path\":\"roberta-base\",\"model_type\":\"roberta\",\"num_train_epochs\":5.0}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transformers-train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"dev\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"label\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"dev\",\"label\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transformers-train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-21-16-24-29-699/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"dev\":\"/opt/ml/input/data/dev\",\"label\":\"/opt/ml/input/data/label\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_name_or_path\":\"roberta-base\",\"model_type\":\"roberta\",\"num_train_epochs\":5.0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"dev\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"label\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-02-21-16-24-29-699\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-21-16-24-29-699/source/sourcedir.tar.gz\",\"module_name\":\"transformers-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transformers-train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_name_or_path\",\"roberta-base\",\"--model_type\",\"roberta\",\"--num_train_epochs\",\"5.0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DEV=/opt/ml/input/data/dev\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_LABEL=/opt/ml/input/data/label\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=5.0\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_TYPE=roberta\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=roberta-base\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python transformers-train.py --model_name_or_path roberta-base --model_type roberta --num_train_epochs 5.0\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m['run_ner.py', '--do_train', '--do_eval', '--evaluate_during_train', '--data_dir', '/opt/ml/input/data/train', '--output_dir', '/opt/ml/model', '--label', '/opt/ml/input/data/label/label.txt', '--model_name_or_path', 'roberta-base', '--model_type', 'roberta', '--num_train_epochs', '5.0']\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:35 - WARNING - run_ner -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.208 algo-1:53 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:36 - INFO - filelock -   Lock 140584984379856 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.208 algo-1:53 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:36 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp55mcsxeo\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.209 algo-1:53 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:37 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.211 algo-1:53 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:37 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.336 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:37 - INFO - filelock -   Lock 140584984379856 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.336 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:37 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.336 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:37 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.350 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0.attention NoneType\n",
      "  \"architectures\": [\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.435 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\n",
      "    \"RobertaForMaskedLM\"\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.435 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\n",
      "  ],\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.435 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.0 NoneType\n",
      "  \"attention_probs_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.502 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\n",
      "  \"bos_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.502 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\n",
      "  \"do_sample\": false,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.502 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention.self NoneType\n",
      "  \"eos_token_ids\": 0,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.518 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1.attention NoneType\n",
      "  \"finetuning_task\": null,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.653 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\n",
      "  \"hidden_act\": \"gelu\",\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.654 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\n",
      "  \"hidden_dropout_prob\": 0.1,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.654 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.1 NoneType\n",
      "  \"hidden_size\": 768,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.729 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\n",
      "  \"id2label\": {\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.729 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\n",
      "    \"0\": \"LABEL_0\",\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.729 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention.self NoneType\n",
      "    \"1\": \"LABEL_1\"\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.744 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2.attention NoneType\n",
      "  },\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.822 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\n",
      "  \"initializer_range\": 0.02,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.822 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\n",
      "  \"intermediate_size\": 3072,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.822 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.2 NoneType\n",
      "  \"is_decoder\": false,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.878 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\n",
      "  \"label2id\": {\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.878 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\n",
      "    \"LABEL_0\": 0,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.878 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention.self NoneType\n",
      "    \"LABEL_1\": 1\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.893 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3.attention NoneType\n",
      "  },\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.971 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\n",
      "  \"layer_norm_eps\": 1e-05,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.971 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\n",
      "  \"length_penalty\": 1.0,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:57.971 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.3 NoneType\n",
      "  \"max_length\": 20,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.024 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\n",
      "  \"max_position_embeddings\": 514,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.025 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\n",
      "  \"model_type\": \"roberta\",\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.025 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention.self NoneType\n",
      "  \"num_attention_heads\": 12,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.039 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4.attention NoneType\n",
      "  \"num_beams\": 1,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.119 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\n",
      "  \"num_hidden_layers\": 12,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.119 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\n",
      "  \"num_labels\": 7,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.119 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.4 NoneType\n",
      "  \"num_return_sequences\": 1,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.173 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\n",
      "  \"output_attentions\": false,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.173 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\n",
      "  \"output_hidden_states\": false,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.173 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention.self NoneType\n",
      "  \"output_past\": true,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.188 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5.attention NoneType\n",
      "  \"pad_token_id\": 0,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.267 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\n",
      "  \"pruned_heads\": {},\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.267 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\n",
      "  \"repetition_penalty\": 1.0,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.267 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.5 NoneType\n",
      "  \"temperature\": 1.0,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.321 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6.attention.self NoneType\n",
      "  \"top_k\": 50,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.321 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6.attention.self NoneType\n",
      "  \"top_p\": 1.0,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.321 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6.attention.self NoneType\n",
      "  \"torchscript\": false,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.335 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6.attention NoneType\n",
      "  \"type_vocab_size\": 1,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.413 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6 NoneType\n",
      "  \"use_bfloat16\": false,\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.413 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6 NoneType\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.413 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.467 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7.attention.self NoneType\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.468 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:38 - INFO - filelock -   Lock 140584984379856 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.468 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:38 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpc88zy7es\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.482 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:40 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.562 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:40 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.562 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:40 - INFO - filelock -   Lock 140584984379856 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.562 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:41 - INFO - filelock -   Lock 140584395452200 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.621 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:41 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp11gwfgeu\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.621 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:42 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.621 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:42 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.636 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:42 - INFO - filelock -   Lock 140584395452200 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.716 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:42 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.716 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:42 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.716 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:43 - INFO - filelock -   Lock 140584395450856 acquired on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.770 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:27:43 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp6zou25dp\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.770 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:52 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.770 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:52 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.785 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:52 - INFO - filelock -   Lock 140584395450856 released on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.864 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:52 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.864 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - transformers.modeling_utils -   Weights of RobertaForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.864 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.918 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/opt/ml/input/data/train', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_predict=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, labels='/opt/ml/input/data/label/label.txt', learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=0, no_cuda=False, num_train_epochs=5.0, output_dir='/opt/ml/model', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=500, seed=42, server_ip='', server_port='', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.918 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -   Creating features from dataset file at /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.918 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   Writing example 0 of 6\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:58.933 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.011 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   guid: train-1\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.011 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   tokens: <s> It 's the thought that count s , but our Online Returns Center makes g ift ex changes and return s simple ( just in case ! ) https : ( link : https :// am zn . to / 2 l 6 q Y K G ) am zn . to / 2 l 6 q Y K G </s> </s>\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.011 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_ids: 0 243 18 627 26086 6025 11432 29 6 4297 2126 19449 48826 25636 39082 571 9417 3463 34980 463 30921 29 41918 1640 8987 179 11173 328 43 13082 35 1640 12139 35 13082 640 424 30083 4 560 73 176 462 401 1343 975 530 534 43 424 30083 4 560 73 176 462 401 1343 975 530 534 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.067 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.067 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.067 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 6 -100 6 6 6 0 3 3 6 6 -100 6 -100 6 6 -100 6 6 6 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.082 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.160 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   guid: train-2\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.161 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   tokens: <s> Did you know you can trade in select Apple , Samsung , and other table ts ? With the Amazon Trade - in program , you can re ceive an Amazon G ift Card + 25 % off t oward a new Fire table t when you trade in your used table ts . ( link : https :// am zn . to / 2 Y b du 1 Y ) am zn . to / 2 Y b du 1 Y </s> </s>\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:28:59.161 algo-1:53 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:roberta.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_ids: 0 20328 6968 27066 6968 7424 18582 179 38450 20770 6 35932 6 463 7443 14595 1872 116 3908 627 25146 35996 12 179 28644 6 6968 7424 241 6550 260 25146 534 9417 23818 2744 1244 207 1529 90 36750 102 4651 17260 14595 90 14746 6968 18582 179 16625 6199 14595 1872 4 1640 12139 35 13082 640 424 30083 4 560 73 176 975 428 6588 134 975 43 424 30083 4 560 73 176 975 428 6588 134 975 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m[2020-02-21 16:29:35.974 algo-1:53 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 6 6 6 0 6 0 6 6 6 6 -100 6 6 6 0 6 6 6 6 6 6 6 6 -100 6 0 6 -100 6 6 6 6 6 6 -100 6 6 0 6 -100 6 6 6 6 6 6 6 -100 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   guid: train-3\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   tokens: <s> Looking to take your phot ography sk ills to the next level ? Check out ( link : http :// amazon . com / prim eday ) amazon . com / prim eday for an am azing camera deal . </s> </s>\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_ids: 0 17629 560 16111 16625 36051 10486 7771 5622 560 627 25616 4483 116 26615 995 1640 12139 35 8166 640 43358 4 175 73 31044 43975 43 43358 4 175 73 31044 43975 1990 260 424 22736 25092 14861 4 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 -100 6 -100 6 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 6 6 6 -100 6 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   guid: train-4\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   tokens: <s> Is a TV on your # Prime Day w ish list ? Keep your eyes on ( link : http :// amazon . com / prim eday ) amazon . com / prim eday for a TV deal soon . </s> </s>\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_ids: 0 6209 102 2915 261 16625 10431 25973 10781 605 1173 8458 116 28407 16625 36078 261 1640 12139 35 8166 640 43358 4 175 73 31044 43975 43 43358 4 175 73 31044 43975 1990 102 2915 14861 16752 4 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 1 4 -100 6 -100 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 6 6 6 6 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   guid: train-5\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   tokens: <s> # Prime Day ends ton ight , but the par ade of deals is still going strong . Get these deals while they 're still hot ! ( link : https :// am zn . to / 2 l g q Z M 3 ) am zn . to / 2 l g q Z M 3 </s> </s>\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_ids: 0 10431 25973 10781 8845 1054 5971 6 4297 627 5489 1829 1116 45794 354 17830 12891 8355 4 14181 29902 45794 20235 10010 214 17830 10120 328 1640 12139 35 13082 640 424 30083 4 560 73 176 462 571 1343 1301 448 246 43 424 30083 4 560 73 176 462 571 1343 1301 448 246 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - utils_ner -   label_ids: -100 1 4 -100 6 6 -100 6 6 6 6 -100 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -   Saving features into cached file /opt/ml/input/data/train/cached_train_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -   ***** Running training *****\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -     Num examples = 6\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -     Num Epochs = 5\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -     Instantaneous batch size per GPU = 8\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -     Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -     Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m02/21/2020 16:28:57 - INFO - run_ner -     Total optimization steps = 5\u001b[0m\n",
      "\u001b[34m#015Epoch:   0% 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:07<00:00,  7.07s/it]#033[A#015Iteration: 100% 1/1 [00:07<00:00,  7.07s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  20% 1/5 [00:07<00:28,  7.07s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:06<00:00,  6.50s/it]#033[A#015Iteration: 100% 1/1 [00:06<00:00,  6.50s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  40% 2/5 [00:13<00:20,  6.90s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:06<00:00,  6.23s/it]#033[A#015Iteration: 100% 1/1 [00:06<00:00,  6.23s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  60% 3/5 [00:19<00:13,  6.70s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:05<00:00,  5.87s/it]#033[A#015Iteration: 100% 1/1 [00:05<00:00,  5.87s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  80% 4/5 [00:25<00:06,  6.45s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:05<00:00,  5.96s/it]#033[A#015Iteration: 100% 1/1 [00:05<00:00,  5.96s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch: 100% 5/5 [00:31<00:00,  6.30s/it]#015Epoch: 100% 5/5 [00:31<00:00,  6.33s/it]\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:28 - INFO - run_ner -    global_step = 5, average loss = 1.6172551155090331\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:28 - INFO - run_ner -   Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:28 - INFO - transformers.configuration_utils -   Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.modeling_utils -   Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.tokenization_utils -   Model name '/opt/ml/model' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/opt/ml/model' is a path, a model identifier, or url to a directory containing tokenizer files.\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.tokenization_utils -   Didn't find file /opt/ml/model/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/vocab.json\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/merges.txt\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.tokenization_utils -   loading file None\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - run_ner -   Evaluate the following checkpoints: ['/opt/ml/model']\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.configuration_utils -   loading configuration file /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:30 - INFO - transformers.modeling_utils -   loading weights file /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -   Creating features from dataset file at /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   Writing example 0 of 3\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   guid: dev-1\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   tokens: <s> Thank you , Prime members , for making this # Prime Day the largest sh opping event in our history ! You p urch ased more than 175 million items , from devices to gro cer ies ! </s> </s>\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   input_ids: 0 13987 6968 6 25973 23742 6 1990 5349 9226 10431 25973 10781 627 8377 1193 26307 21680 179 2126 37283 328 1185 642 9222 11835 4321 5652 16925 4416 46740 6 7761 44326 560 15821 7742 918 328 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   label_ids: -100 6 6 6 0 6 6 6 6 6 1 4 -100 6 6 6 -100 6 6 6 6 6 6 6 -100 -100 6 6 6 6 6 6 6 6 6 6 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   guid: dev-2\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   tokens: <s> Improve your mus ical tal ents by st aying in t une on ( link : http :// amazon . com / prim eday ) amazon . com / prim eday for an ac oustic g uit ar deal laun ching soon . </s> </s>\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   input_ids: 0 47302 16625 13792 3569 29420 4189 1409 620 11918 179 90 4438 261 1640 12139 35 8166 640 43358 4 175 73 31044 43975 43 43358 4 175 73 31044 43975 1990 260 1043 38645 571 6439 271 14861 30866 7968 16752 4 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   label_ids: -100 6 6 6 -100 6 -100 6 6 -100 6 6 -100 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 6 6 6 -100 6 -100 -100 6 6 -100 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   guid: dev-3\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   tokens: <s> \" I can tell you very sen ior CEO s of major American car compan ies would shake my hand and turn away because I was n 't worth talking to , \" said Th run , in an inter view with Rec ode ear lier this week . </s> </s>\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   input_ids: 0 113 100 7424 26316 6968 5525 7305 7375 24205 29 1116 25430 4310 5901 39788 918 14656 41820 4783 4539 463 15922 4384 13437 100 7325 282 75 5985 34982 560 6 113 28613 11329 2962 6 179 260 8007 5877 5632 21109 4636 4352 14730 9226 3583 4 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 6 6 -100 6 -100 6 6 0 6 6 -100 6 6 6 6 6 6 6 6 6 6 6 -100 6 6 6 6 6 6 2 -100 6 6 6 6 -100 6 0 -100 6 -100 6 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -   Saving features into cached file /opt/ml/input/data/train/cached_dev_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -   ***** Running evaluation  *****\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -     Num examples = 3\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -     Batch size = 8\u001b[0m\n",
      "\u001b[34m#015Evaluating:   0% 0/1 [00:00<?, ?it/s]#015Evaluating: 100% 1/1 [00:00<00:00,  1.39it/s]#015Evaluating: 100% 1/1 [00:00<00:00,  1.39it/s]\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -   ***** Eval results  *****\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -     f1 = 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -     loss = 1.2197113037109375\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -     precision = 0\u001b[0m\n",
      "\u001b[34m02/21/2020 16:29:35 - INFO - run_ner -     recall = 0.0\u001b[0m\n",
      "\u001b[34m2020-02-21 16:29:36,417 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-02-21 16:29:37 Uploading - Uploading generated training model\n",
      "2020-02-21 16:31:45 Completed - Training job completed\n",
      "Training seconds: 323\n",
      "Billable seconds: 323\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train': s3_input(train), 'dev': s3_input(dev), 'label': s3_input(label)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
