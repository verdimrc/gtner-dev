{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='font-size:250%; font-weight:bold'>Train NER with huggingface/transformers</div>\n",
    "\n",
    "This notebook shows how to use `huggingface/transformers` on Amazon SageMaker to transfer-learn the Roberta language model into a new NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "import s3fs\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "from gtner_blog.util import split, bilou2bio, write_split, LabelCollector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download transformers NER scripts\n",
    "\n",
    "The `huggingface/transformers` repo contains two PyTorch scripts to download, namely `run_ner.py` and `utils_ner.py`. The following `bash` cell downloads version v2.5.0 which matches the library listed in `requirements.txt`.\n",
    "\n",
    "To minimize the dependencies installed to the training container, we also download the `seqeval` into `source_dir/` to emulate `pip install --no-deps seqeval`. The `seqeval.callbacks` depends on [Keras](https://github.com/chakki-works/seqeval/blob/v0.0.12/seqeval/callbacks.py) and [tensorflow](https://github.com/chakki-works/seqeval/blob/v0.0.12/requirements.txt), but `run_ner.py` does not uses these callbacks (and only `seqeval.metrics`), hence both dependencies can be [skipped](https://github.com/chakki-works/seqeval/blob/v0.0.12/seqeval/metrics/sequence_labeling.py).\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Note</summary>\n",
    "    <blockquote>As of this writing, the master branch of <code>huggingface/transformers</code> has relocated the NER scripts from <code>examples/</code> to <code>examples/ner/</code>, which is beyond the scope of this notebook.</blockquote>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ec2-user ec2-user    32 Feb 21 07:38 requirements.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 30349 Feb 27 13:28 \u001b[01;31m\u001b[Krun_ner.py\u001b[m\u001b[K\n",
      "drwxrwxr-x 3 ec2-user ec2-user  4096 Feb 21 08:12 \u001b[01;31m\u001b[Kseqeval\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  3111 Feb 27 13:28 \u001b[01;31m\u001b[Kseqeval/callbacks.py\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user     0 Feb 27 13:28 \u001b[01;31m\u001b[Kseqeval/__init__.py\u001b[m\u001b[K\n",
      "drwxrwxr-x 2 ec2-user ec2-user  4096 Feb 21 08:12 \u001b[01;31m\u001b[Kseqeval/metrics\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   371 Feb 27 13:28 \u001b[01;31m\u001b[Kseqeval/metrics/__init__.py\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 12604 Feb 27 13:28 \u001b[01;31m\u001b[Kseqeval/metrics/sequence_labeling.py\u001b[m\u001b[K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  5121 Feb 27 13:28 transformers-train.py\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  8428 Feb 27 13:28 \u001b[01;31m\u001b[Kutils_ner.py\u001b[m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "GITHUB=https://raw.githubusercontent.com\n",
    "cd transformers-scripts\n",
    "\n",
    "# Download NER scripts\n",
    "for i in run_ner.py utils_ner.py\n",
    "do\n",
    "    curl --silent --location $GITHUB/huggingface/transformers/v2.5.0/examples/$i > $i\n",
    "done\n",
    "\n",
    "# Download seqeval\n",
    "mkdir -p seqeval/metrics\n",
    "for i in __init__.py callbacks.py metrics/__init__.py metrics/sequence_labeling.py\n",
    "do\n",
    "    curl --silent --location $GITHUB/chakki-works/seqeval/v0.0.12/seqeval/$i > seqeval/$i\n",
    "done\n",
    "\n",
    "ls -ald * seqeval/* seqeval/metrics/* | egrep --color=always 'run_ner.py|utils_ner.py|seqeval.*|^'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data channels\n",
    "\n",
    "Split the whole corpus in S3 into train:test = 3:1 proportion, then upload the splits to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/gt/test-gtner-blog-004/manifests/output/output.iob'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/transformers-data/train'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/transformers-data/dev'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3://gtner-blog/transformers-data/label'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bucket = 'gtner-blog'                # Change me as necessary\n",
    "gt_jobname = 'test-gtner-blog-004'   # Change me as necessary\n",
    "\n",
    "iob_file = f's3://{bucket}/gt/{gt_jobname}/manifests/output/output.iob'\n",
    "train = f's3://{bucket}/transformers-data/train'\n",
    "dev = f's3://{bucket}/transformers-data/dev'\n",
    "label = f's3://{bucket}/transformers-data/label'\n",
    "label_collector = LabelCollector()\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "with fs.open(iob_file, 'r') as f:\n",
    "    train_split = os.path.join(train, 'train.txt')\n",
    "    dev_split = os.path.join(dev, 'dev.txt')\n",
    "    \n",
    "    # Chain of functions: .iob -> bilou2bio -> label_collector -> split -> write_split.\n",
    "    write_split(split(label_collector(bilou2bio(f))), train_split, dev_split)\n",
    "\n",
    "with fs.open(os.path.join(label, 'label.txt'), 'w') as f:\n",
    "    for ner_tag in label_collector.sorted_labels:\n",
    "        f.write(f'{ner_tag}\\n')\n",
    "\n",
    "display(iob_file, train, dev, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training\n",
    "\n",
    "We create a PyTorch estimator with our entry point script `transformers-train.py`, a thin wrapper over `run_ner.py` that does the following:\n",
    "\n",
    "1. parse SageMaker's entry-point protocol, namely model and channel directories.\n",
    "2. pre-define a few arguments to `run-ner.py`: `{\"--do_train\", \"--do-eval\", \"--evaluate_during_train\", \"--data_dir\", \"--output_dir\", \"--label\"}`.\n",
    "3. passes the estimator's hyper-parameters as arguments to `run-ner.py`.\n",
    "   1. Each hyperparameter `abcd` will be passed down as `--abcd`.\n",
    "   2. The hyperparameters must not conflict with those in the above mentioned step 2.\n",
    "   3. The entry point only support `--abcd SOME_VALUE` form of arguments.\n",
    "\n",
    "<details>\n",
    "    <summary><code>python run_ner.py -h</code></summary>\n",
    "    <blockquote><pre>\n",
    "usage: run_ner.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\n",
    "                  --model_name_or_path MODEL_NAME_OR_PATH --output_dir\n",
    "                  OUTPUT_DIR [--labels LABELS] [--config_name CONFIG_NAME]\n",
    "                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
    "                  [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\n",
    "                  [--do_predict] [--evaluate_during_training]\n",
    "                  [--do_lower_case]\n",
    "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
    "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
    "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
    "                  [--learning_rate LEARNING_RATE]\n",
    "                  [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n",
    "                  [--max_grad_norm MAX_GRAD_NORM]\n",
    "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
    "                  [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n",
    "                  [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
    "                  [--eval_all_checkpoints] [--no_cuda]\n",
    "                  [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n",
    "                  [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n",
    "                  [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n",
    "                  [--server_port SERVER_PORT]\n",
    "</pre></blockquote>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter logging_steps=1 causes stats to be written to tensorboard event file\n",
    "# every 1 minibatch. The default is 500 which may not generate enough content for this demo.\n",
    "estimator = PyTorch(entry_point='transformers-train.py',\n",
    "                    source_dir='./transformers-scripts',\n",
    "                    role=get_execution_role(),\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m5.large',\n",
    "                    framework_version='1.3.1',\n",
    "                    py_version='py3',\n",
    "                    debugger_hook_config=False,\n",
    "                    hyperparameters={\n",
    "                        'num_train_epochs': 5.0,\n",
    "                        'model_type': 'roberta',\n",
    "                        'model_name_or_path': 'roberta-base',\n",
    "                        'logging_steps': 1\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will start a training job. The training job will download a few hundred MB of pretrained model, and this may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-27 13:28:54 Starting - Starting the training job...\n",
      "2020-02-27 13:28:56 Starting - Launching requested ML instances......\n",
      "2020-02-27 13:29:59 Starting - Preparing the instances for training...\n",
      "2020-02-27 13:30:48 Downloading - Downloading input data...\n",
      "2020-02-27 13:31:23 Training - Downloading the training image...\n",
      "2020-02-27 13:31:45 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:46,617 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:46,620 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:46,631 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:49,718 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:49,944 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:49,944 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:49,944 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:49,945 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpedtkkspi/module_dir\u001b[0m\n",
      "\u001b[34mCollecting transformers==2.5.0\n",
      "  Downloading transformers-2.5.0-py3-none-any.whl (481 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\n",
      "  Downloading tensorboard-2.1.0-py3-none-any.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (1.11.7)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (4.36.1)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.2.20-cp36-cp36m-manylinux2010_x86_64.whl (690 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.5.0->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.5.0\n",
      "  Downloading tokenizers-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (1.12.0)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (0.33.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (0.15.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (44.0.0.post20200106)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.11.2-py2.py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.1-py2.py3-none-any.whl (88 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard->-r requirements.txt (line 2)) (3.11.2)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl (2.7 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.15.0,>=1.14.7 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.5.0->-r requirements.txt (line 1)) (1.14.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.5.0->-r requirements.txt (line 1)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.5.0->-r requirements.txt (line 1)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.5.0->-r requirements.txt (line 1)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.5.0->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.5.0->-r requirements.txt (line 1)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (3.4.2)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.5.0->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.5.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 2)) (0.4.8)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, sacremoses, absl-py\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=36524 sha256=f9a32a32cd77756aac19689a76b04cd5d17e7d98279f19d8b366826106dd2980\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fpfsyqpw/wheels/04/c0/3f/3490cfed93198ab7d2cf64538be821e4c2f6ea990011a94931\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=4f44e4b5feb33c9baed1a0e70b7d74551200d3f2ff33f4e2aa68c76944717fa6\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\n",
      "  Building wheel for absl-py (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=7d9acbdfe43a9b8248ed7040840f8ff0610a26e12a21497f3d1d97d0bb6ed9a3\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name sacremoses absl-py\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, regex, sacremoses, filelock, tokenizers, transformers, oauthlib, requests-oauthlib, cachetools, pyasn1-modules, google-auth, google-auth-oauthlib, absl-py, markdown, grpcio, tensorboard, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-0.9.0 cachetools-4.0.0 default-user-module-name-1.0.0 filelock-3.0.12 google-auth-1.11.2 google-auth-oauthlib-0.4.1 grpcio-1.27.2 markdown-3.2.1 oauthlib-3.1.0 pyasn1-modules-0.2.8 regex-2020.2.20 requests-oauthlib-1.3.0 sacremoses-0.0.38 sentencepiece-0.1.85 tensorboard-2.1.0 tokenizers-0.5.0 transformers-2.5.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:57,910 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:57,923 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:57,936 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-02-27 13:31:57,947 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"dev\": \"/opt/ml/input/data/dev\",\n",
      "        \"label\": \"/opt/ml/input/data/label\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"num_train_epochs\": 5.0,\n",
      "        \"model_type\": \"roberta\",\n",
      "        \"logging_steps\": 1,\n",
      "        \"model_name_or_path\": \"roberta-base\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"dev\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"label\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-02-27-13-28-53-911\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-27-13-28-53-911/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transformers-train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transformers-train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"logging_steps\":1,\"model_name_or_path\":\"roberta-base\",\"model_type\":\"roberta\",\"num_train_epochs\":5.0}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transformers-train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"dev\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"label\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"dev\",\"label\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transformers-train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-27-13-28-53-911/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"dev\":\"/opt/ml/input/data/dev\",\"label\":\"/opt/ml/input/data/label\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"logging_steps\":1,\"model_name_or_path\":\"roberta-base\",\"model_type\":\"roberta\",\"num_train_epochs\":5.0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"dev\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"label\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-02-27-13-28-53-911\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-27-13-28-53-911/source/sourcedir.tar.gz\",\"module_name\":\"transformers-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transformers-train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--logging_steps\",\"1\",\"--model_name_or_path\",\"roberta-base\",\"--model_type\",\"roberta\",\"--num_train_epochs\",\"5.0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DEV=/opt/ml/input/data/dev\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_LABEL=/opt/ml/input/data/label\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=5.0\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_TYPE=roberta\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_STEPS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=roberta-base\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python transformers-train.py --logging_steps 1 --model_name_or_path roberta-base --model_type roberta --num_train_epochs 5.0\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m['run_ner.py', '--do_train', '--do_eval', '--evaluate_during_train', '--data_dir', '/opt/ml/input/data/train', '--output_dir', '/opt/ml/model', '--label', '/opt/ml/input/data/label/label.txt', '--logging_steps', '1', '--model_name_or_path', 'roberta-base', '--model_type', 'roberta', '--num_train_epochs', '5.0']\u001b[0m\n",
      "\u001b[34m02/27/2020 13:31:59 - WARNING - run_ner -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:00 - INFO - filelock -   Lock 139728824594840 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:00 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpk41l6u8w\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:01 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:01 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:01 - INFO - filelock -   Lock 139728824594840 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:01 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:01 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:02 - INFO - filelock -   Lock 139728824594840 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:02 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp9pwlfejt\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:04 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:04 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:04 - INFO - filelock -   Lock 139728824594840 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:05 - INFO - filelock -   Lock 139728824594840 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:05 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp1k2s47si\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:07 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:07 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:07 - INFO - filelock -   Lock 139728824594840 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:07 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:08 - INFO - filelock -   Lock 139728824594840 acquired on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m02/27/2020 13:32:08 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmppoad54pa\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:17 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:17 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:17 - INFO - filelock -   Lock 139728824594840 released on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:17 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - transformers.modeling_utils -   Weights of RobertaForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/opt/ml/input/data/train', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_predict=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, labels='/opt/ml/input/data/label/label.txt', learning_rate=5e-05, local_rank=-1, logging_steps=1, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=0, no_cuda=False, num_train_epochs=5.0, output_dir='/opt/ml/model', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=500, seed=42, server_ip='', server_port='', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -   Creating features from dataset file at /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   Writing example 0 of 6\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   guid: train-1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   tokens: <s> It 's the thought that count s , but our Online Returns Center makes g ift ex changes and return s simple ( just in case ! ) https : ( link : https :// am zn . to / 2 l 6 q Y K G ) am zn . to / 2 l 6 q Y K G </s> </s>\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_ids: 0 243 18 627 26086 6025 11432 29 6 4297 2126 19449 48826 25636 39082 571 9417 3463 34980 463 30921 29 41918 1640 8987 179 11173 328 43 13082 35 1640 12139 35 13082 640 424 30083 4 560 73 176 462 401 1343 975 530 534 43 424 30083 4 560 73 176 462 401 1343 975 530 534 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 6 -100 6 6 6 0 3 3 6 6 -100 6 -100 6 6 -100 6 6 6 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   guid: train-2\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   tokens: <s> Did you know you can trade in select Apple , Samsung , and other table ts ? With the Amazon Trade - in program , you can re ceive an Amazon G ift Card + 25 % off t oward a new Fire table t when you trade in your used table ts . ( link : https :// am zn . to / 2 Y b du 1 Y ) am zn . to / 2 Y b du 1 Y </s> </s>\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_ids: 0 20328 6968 27066 6968 7424 18582 179 38450 20770 6 35932 6 463 7443 14595 1872 116 3908 627 25146 35996 12 179 28644 6 6968 7424 241 6550 260 25146 534 9417 23818 2744 1244 207 1529 90 36750 102 4651 17260 14595 90 14746 6968 18582 179 16625 6199 14595 1872 4 1640 12139 35 13082 640 424 30083 4 560 73 176 975 428 6588 134 975 43 424 30083 4 560 73 176 975 428 6588 134 975 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 6 6 6 0 6 0 6 6 6 6 -100 6 6 6 0 6 6 6 6 6 6 6 6 -100 6 0 6 -100 6 6 6 6 6 6 -100 6 6 0 6 -100 6 6 6 6 6 6 6 -100 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   guid: train-3\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   tokens: <s> Looking to take your phot ography sk ills to the next level ? Check out ( link : http :// amazon . com / prim eday ) amazon . com / prim eday for an am azing camera deal . </s> </s>\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_ids: 0 17629 560 16111 16625 36051 10486 7771 5622 560 627 25616 4483 116 26615 995 1640 12139 35 8166 640 43358 4 175 73 31044 43975 43 43358 4 175 73 31044 43975 1990 260 424 22736 25092 14861 4 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 -100 6 -100 6 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 6 6 6 -100 6 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   guid: train-4\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   tokens: <s> Is a TV on your # Prime Day w ish list ? Keep your eyes on ( link : http :// amazon . com / prim eday ) amazon . com / prim eday for a TV deal soon . </s> </s>\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_ids: 0 6209 102 2915 261 16625 10431 25973 10781 605 1173 8458 116 28407 16625 36078 261 1640 12139 35 8166 640 43358 4 175 73 31044 43975 43 43358 4 175 73 31044 43975 1990 102 2915 14861 16752 4 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 1 4 -100 6 -100 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 6 6 6 6 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   guid: train-5\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   tokens: <s> # Prime Day ends ton ight , but the par ade of deals is still going strong . Get these deals while they 're still hot ! ( link : https :// am zn . to / 2 l g q Z M 3 ) am zn . to / 2 l g q Z M 3 </s> </s>\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_ids: 0 10431 25973 10781 8845 1054 5971 6 4297 627 5489 1829 1116 45794 354 17830 12891 8355 4 14181 29902 45794 20235 10010 214 17830 10120 328 1640 12139 35 13082 640 424 30083 4 560 73 176 462 571 1343 1301 448 246 43 424 30083 4 560 73 176 462 571 1343 1301 448 246 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - utils_ner -   label_ids: -100 1 4 -100 6 6 -100 6 6 6 6 -100 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -   Saving features into cached file /opt/ml/input/data/train/cached_train_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -   ***** Running training *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -     Num examples = 6\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -     Num Epochs = 5\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -     Instantaneous batch size per GPU = 8\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -     Total train batch size (w. parallel, distributed & accumulation) = 8\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -     Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:22 - INFO - run_ner -     Total optimization steps = 5\u001b[0m\n",
      "\u001b[34m#015Epoch:   0% 0/5 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -   Creating features from dataset file at /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   Writing example 0 of 3\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   guid: dev-1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   tokens: <s> Thank you , Prime members , for making this # Prime Day the largest sh opping event in our history ! You p urch ased more than 175 million items , from devices to gro cer ies ! </s> </s>\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   input_ids: 0 13987 6968 6 25973 23742 6 1990 5349 9226 10431 25973 10781 627 8377 1193 26307 21680 179 2126 37283 328 1185 642 9222 11835 4321 5652 16925 4416 46740 6 7761 44326 560 15821 7742 918 328 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   label_ids: -100 6 6 6 0 6 6 6 6 6 1 4 -100 6 6 6 -100 6 6 6 6 6 6 6 -100 -100 6 6 6 6 6 6 6 6 6 6 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   guid: dev-2\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   tokens: <s> Improve your mus ical tal ents by st aying in t une on ( link : http :// amazon . com / prim eday ) amazon . com / prim eday for an ac oustic g uit ar deal laun ching soon . </s> </s>\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   input_ids: 0 47302 16625 13792 3569 29420 4189 1409 620 11918 179 90 4438 261 1640 12139 35 8166 640 43358 4 175 73 31044 43975 43 43358 4 175 73 31044 43975 1990 260 1043 38645 571 6439 271 14861 30866 7968 16752 4 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   label_ids: -100 6 6 6 -100 6 -100 6 6 -100 6 6 -100 6 6 6 6 1 -100 -100 -100 -100 -100 -100 -100 6 1 -100 -100 -100 -100 -100 6 6 6 -100 6 -100 -100 6 6 -100 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   *** Example ***\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   guid: dev-3\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   tokens: <s> \" I can tell you very sen ior CEO s of major American car compan ies would shake my hand and turn away because I was n 't worth talking to , \" said Th run , in an inter view with Rec ode ear lier this week . </s> </s>\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   input_ids: 0 113 100 7424 26316 6968 5525 7305 7375 24205 29 1116 25430 4310 5901 39788 918 14656 41820 4783 4539 463 15922 4384 13437 100 7325 282 75 5985 34982 560 6 113 28613 11329 2962 6 179 260 8007 5877 5632 21109 4636 4352 14730 9226 3583 4 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - utils_ner -   label_ids: -100 6 6 6 6 6 6 6 -100 6 -100 6 6 0 6 6 -100 6 6 6 6 6 6 6 6 6 6 6 -100 6 6 6 6 6 6 2 -100 6 6 6 6 -100 6 0 -100 6 -100 6 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -   Saving features into cached file /opt/ml/input/data/train/cached_dev_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -   ***** Running evaluation  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -     Num examples = 3\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -     Batch size = 8\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating:   0% 0/1 [00:00<?, ?it/s]#033[A#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating: 100% 1/1 [00:00<00:00,  1.53it/s]#033[A#033[A#015Evaluating: 100% 1/1 [00:00<00:00,  1.53it/s]\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -   ***** Eval results  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -     f1 = 0.06249999999999999\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -     loss = 1.898728370666504\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -     precision = 0.033707865168539325\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:29 - INFO - run_ner -     recall = 0.42857142857142855\n",
      "\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:07<00:00,  7.67s/it]#033[A#015Iteration: 100% 1/1 [00:07<00:00,  7.67s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  20% 1/5 [00:07<00:30,  7.67s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A02/27/2020 13:33:36 - INFO - run_ner -   Loading features from cached file /opt/ml/input/data/train/cached_dev_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:36 - INFO - run_ner -   ***** Running evaluation  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:36 - INFO - run_ner -     Num examples = 3\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:36 - INFO - run_ner -     Batch size = 8\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating:   0% 0/1 [00:00<?, ?it/s]#033[A#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating: 100% 1/1 [00:00<00:00,  1.53it/s]#033[A#033[A#015Evaluating: 100% 1/1 [00:00<00:00,  1.53it/s]\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:37 - INFO - run_ner -   ***** Eval results  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:37 - INFO - run_ner -     f1 = 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:37 - INFO - run_ner -     loss = 1.6086888313293457\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:37 - INFO - run_ner -     precision = 0.0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:37 - INFO - run_ner -     recall = 0.0\n",
      "\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:07<00:00,  7.11s/it]#033[A#015Iteration: 100% 1/1 [00:07<00:00,  7.11s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  40% 2/5 [00:14<00:22,  7.50s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A02/27/2020 13:33:43 - INFO - run_ner -   Loading features from cached file /opt/ml/input/data/train/cached_dev_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:43 - INFO - run_ner -   ***** Running evaluation  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:43 - INFO - run_ner -     Num examples = 3\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:43 - INFO - run_ner -     Batch size = 8\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating:   0% 0/1 [00:00<?, ?it/s]#033[A#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating: 100% 1/1 [00:00<00:00,  1.50it/s]#033[A#033[A#015Evaluating: 100% 1/1 [00:00<00:00,  1.50it/s]\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:43 - INFO - run_ner -   ***** Eval results  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:43 - INFO - run_ner -     f1 = 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:43 - INFO - run_ner -     loss = 1.3646026849746704\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:43 - INFO - run_ner -     precision = 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:43 - INFO - run_ner -     recall = 0.0\n",
      "\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:06<00:00,  6.82s/it]#033[A#015Iteration: 100% 1/1 [00:06<00:00,  6.82s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  60% 3/5 [00:21<00:14,  7.30s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A02/27/2020 13:33:49 - INFO - run_ner -   Loading features from cached file /opt/ml/input/data/train/cached_dev_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:49 - INFO - run_ner -   ***** Running evaluation  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:49 - INFO - run_ner -     Num examples = 3\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:49 - INFO - run_ner -     Batch size = 8\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating:   0% 0/1 [00:00<?, ?it/s]#033[A#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating: 100% 1/1 [00:00<00:00,  1.53it/s]#033[A#033[A#015Evaluating: 100% 1/1 [00:00<00:00,  1.53it/s]\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:50 - INFO - run_ner -   ***** Eval results  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:50 - INFO - run_ner -     f1 = 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:50 - INFO - run_ner -     loss = 1.2132974863052368\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:50 - INFO - run_ner -     precision = 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:50 - INFO - run_ner -     recall = 0.0\n",
      "\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:06<00:00,  6.52s/it]#033[A#015Iteration: 100% 1/1 [00:06<00:00,  6.52s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch:  80% 4/5 [00:28<00:07,  7.06s/it]\u001b[0m\n",
      "\u001b[34m#015Iteration:   0% 0/1 [00:00<?, ?it/s]#033[A02/27/2020 13:33:56 - INFO - run_ner -   Loading features from cached file /opt/ml/input/data/train/cached_dev_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:56 - INFO - run_ner -   ***** Running evaluation  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:56 - INFO - run_ner -     Num examples = 3\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:56 - INFO - run_ner -     Batch size = 8\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating:   0% 0/1 [00:00<?, ?it/s]#033[A#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Evaluating: 100% 1/1 [00:00<00:00,  1.51it/s]#033[A#033[A#015Evaluating: 100% 1/1 [00:00<00:00,  1.51it/s]\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:57 - INFO - run_ner -   ***** Eval results  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:57 - INFO - run_ner -     f1 = 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:57 - INFO - run_ner -     loss = 1.2132974863052368\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:57 - INFO - run_ner -     precision = 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:57 - INFO - run_ner -     recall = 0.0\n",
      "\u001b[0m\n",
      "\u001b[34m#015Iteration: 100% 1/1 [00:06<00:00,  6.62s/it]#033[A#015Iteration: 100% 1/1 [00:06<00:00,  6.62s/it]\u001b[0m\n",
      "\u001b[34m#015Epoch: 100% 5/5 [00:34<00:00,  6.93s/it]#015Epoch: 100% 5/5 [00:34<00:00,  6.95s/it]\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:57 - INFO - run_ner -    global_step = 5, average loss = 1.6025830507278442\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:57 - INFO - run_ner -   Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:57 - INFO - transformers.configuration_utils -   Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.modeling_utils -   Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.tokenization_utils -   Model name '/opt/ml/model' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/opt/ml/model' is a path, a model identifier, or url to a directory containing tokenizer files.\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.tokenization_utils -   Didn't find file /opt/ml/model/added_tokens.json. We won't load it.\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/vocab.json\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/merges.txt\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.tokenization_utils -   loading file None\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.tokenization_utils -   loading file /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - run_ner -   Evaluate the following checkpoints: ['/opt/ml/model']\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.configuration_utils -   loading configuration file /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 7,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m02/27/2020 13:33:58 - INFO - transformers.modeling_utils -   loading weights file /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m02/27/2020 13:34:03 - INFO - run_ner -   Loading features from cached file /opt/ml/input/data/train/cached_dev_roberta-base_128\u001b[0m\n",
      "\u001b[34m02/27/2020 13:34:03 - INFO - run_ner -   ***** Running evaluation  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:34:03 - INFO - run_ner -     Num examples = 3\u001b[0m\n",
      "\u001b[34m02/27/2020 13:34:03 - INFO - run_ner -     Batch size = 8\u001b[0m\n",
      "\u001b[34m#015Evaluating:   0% 0/1 [00:00<?, ?it/s]#015Evaluating: 100% 1/1 [00:00<00:00,  1.32it/s]#015Evaluating: 100% 1/1 [00:00<00:00,  1.32it/s]\u001b[0m\n",
      "\u001b[34m02/27/2020 13:34:04 - INFO - run_ner -   ***** Eval results  *****\u001b[0m\n",
      "\u001b[34m02/27/2020 13:34:04 - INFO - run_ner -     f1 = 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:34:04 - INFO - run_ner -     loss = 1.2132974863052368\u001b[0m\n",
      "\u001b[34m02/27/2020 13:34:04 - INFO - run_ner -     precision = 0\u001b[0m\n",
      "\u001b[34m02/27/2020 13:34:04 - INFO - run_ner -     recall = 0.0\u001b[0m\n",
      "\u001b[34m2020-02-27 13:34:04,749 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-02-27 13:34:08 Uploading - Uploading generated training model\n",
      "2020-02-27 13:36:44 Completed - Training job completed\n",
      "Training seconds: 356\n",
      "Billable seconds: 356\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'train': s3_input(train), 'dev': s3_input(dev), 'label': s3_input(label)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize training progress\n",
    "\n",
    "The tranformers's NER script records the training performance to a Tensorboard event file, and our entrypoint script ensures that SageMaker picks it up and upload it as `output.tar.gz`. We're going to install Tensorboard on this notebook instance, then visualize the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract training output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file is s3://sagemaker-ap-southeast-1-484597657167/pytorch-training-2020-02-27-13-28-53-911/output/output.tar.gz\n",
      "Extracted to this instance:\n",
      "runs/:\n",
      "total 12\n",
      "drwxr-xr-x 3 ec2-user ec2-user 4096 Feb 27 13:33 .\n",
      "drwxrwxr-x 6 ec2-user ec2-user 4096 Feb 27 13:48 ..\n",
      "drwxr-xr-x 2 ec2-user ec2-user 4096 Feb 27 13:33 Feb27_13-33-22_algo-1\n",
      "\n",
      "runs/Feb27_13-33-22_algo-1:\n",
      "total 12\n",
      "drwxr-xr-x 2 ec2-user ec2-user 4096 Feb 27 13:33 .\n",
      "drwxr-xr-x 3 ec2-user ec2-user 4096 Feb 27 13:33 ..\n",
      "-rw-r--r-- 1 ec2-user ec2-user 1415 Feb 27 13:33 events.out.tfevents.1582810402.algo-1.55.0\n"
     ]
    }
   ],
   "source": [
    "# Download tensorboard event file to this notebook instance.\n",
    "output_tgz = estimator.model_data.replace('model.tar.gz', 'output.tar.gz')\n",
    "print('Output file is', output_tgz)\n",
    "with fs.open(output_tgz, 'rb') as f:\n",
    "    with tarfile.open(None, 'r', f) as tgz:\n",
    "        tgz.extractall('.')\n",
    "\n",
    "print('Extracted to this instance:')\n",
    "!ls -alR runs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal, then do this:\n",
    "\n",
    "```bash\n",
    "# The conda environment name is python3, which should match the kernel used by this notebook.\n",
    "# If this notebook uses a different kernel, you must activate the matching conda environment.\n",
    "source activate python3\n",
    "tensorboard --logdir=/home/ec2-user/SageMaker/gtner-dev/notebooks/runs\n",
    "```\n",
    "\n",
    "Then, open [this link](/proxy/6006/) in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
